{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_trainer import run_wandb_training, run_training_without_logging\n",
    "from energy_model.energy_model import EnergyModel\n",
    "from model_trainer import Antolik_dataset_preparation_function\n",
    "from utils import get_config\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ENTITY = \"csng-cuni\"\n",
    "PROJECT = \"reCNN_visual_prosthesis\"\n",
    "model = None\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "train_on_test = False\n",
    "max_epochs = 10\n",
    "max_time = 0\n",
    "patience = 7\n",
    "train_on_val = False\n",
    "test = True\n",
    "seed = 42\n",
    "batch_size = 10\n",
    "lr = 0.001\n",
    "\n",
    "def main():\n",
    "\n",
    "    config = get_config(model=\"EM\")\n",
    "\n",
    "    # TRAINING PARAMETERS\n",
    "    config[\"train_on_test\"] = train_on_test\n",
    "    config[\"max_epochs\"] = max_epochs\n",
    "    config[\"max_time\"] = max_time\n",
    "    config[\"patience\"] = patience\n",
    "    config[\"train_on_val\"] = train_on_val\n",
    "    config[\"test\"] = test\n",
    "    config[\"seed\"] = seed\n",
    "    config[\"batch_size\"] = batch_size\n",
    "    config[\"lr\"] = lr\n",
    "    \n",
    "    # MODEL PARAMETERS\n",
    "    # config[\"core_gamma_hidden\"] = 0.008931320307500908\n",
    "    # config[\"bottleneck_kernel\"] = 5\n",
    "    # config[\"core_gamma_input\"] = 0.2384005754453638\n",
    "    # config[\"core_hidden_channels\"] = 5\n",
    "    # config[\"core_hidden_kern\"] = 5\n",
    "    # config[\"core_input_kern\"] = 5\n",
    "    # config[\"core_layers\"] = 4\n",
    "    # config[\"depth_separable\"] = True\n",
    "    # config[\"lr\"] = 0.0005\n",
    "    # config[\"num_rotations\"] = 64\n",
    "    # config[\"upsampling\"] = 1\n",
    "\n",
    "    config[\"train_data_dir\"] = \"/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/one_trials.pickle\"\n",
    "    config[\"test_data_dir\"] = \"/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/ten_trials.pickle\"\n",
    "\n",
    "    model = run_wandb_training(\n",
    "        config,\n",
    "        Antolik_dataset_preparation_function,\n",
    "        ENTITY,\n",
    "        PROJECT,\n",
    "        model_class=EnergyModel\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcsng-cuni\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/2dh95zgb\" target=\"_blank\">daily-field-273</a></strong> to <a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 10,\n",
      " 'bias_init': 2.5,\n",
      " 'bottleneck_kernel': 15,\n",
      " 'brain_crop': None,\n",
      " 'compute_oracle_fraction': False,\n",
      " 'conservative_oracle': True,\n",
      " 'core_gamma_hidden': 0.28463619129195233,\n",
      " 'core_gamma_input': 0.00307424496692959,\n",
      " 'core_hidden_channels': 3,\n",
      " 'core_hidden_kern': 3,\n",
      " 'core_input_kern': 3,\n",
      " 'core_layers': 1,\n",
      " 'counter_clockwise_rotation': True,\n",
      " 'dataset_artifact_name': 'Antolik_dataset:latest',\n",
      " 'default_ori_shift': 90,\n",
      " 'depth_separable': True,\n",
      " 'do_not_sample': True,\n",
      " 'exact_init': True,\n",
      " 'f_init': 0.63,\n",
      " 'factor': 5.5,\n",
      " 'fixed_sigma': False,\n",
      " 'freeze_orientations': False,\n",
      " 'freeze_positions': False,\n",
      " 'generate_oracle_figure': False,\n",
      " 'ground_truth_orientations_file_path': 'data/antolik/oris_reparametrized.pickle',\n",
      " 'ground_truth_positions_file_path': 'data/antolik/positions_reparametrized.pickle',\n",
      " 'init_mu_range': 0.3,\n",
      " 'init_sigma_range': 0.1,\n",
      " 'init_to_ground_truth_orientations': True,\n",
      " 'init_to_ground_truth_positions': True,\n",
      " 'input_regularizer': 'LaplaceL2norm',\n",
      " 'jackknife_oracle': True,\n",
      " 'lr': 0.001,\n",
      " 'max_epochs': 10,\n",
      " 'max_time': 0,\n",
      " 'model_needs_dataloader': False,\n",
      " 'multivariate': True,\n",
      " 'needs_ground_truth': True,\n",
      " 'nonlinearity': 'softplus',\n",
      " 'normalize': True,\n",
      " 'num_rotations': 4,\n",
      " 'observed_val_metric': 'val/corr',\n",
      " 'orientation_shift': 87.4,\n",
      " 'patience': 7,\n",
      " 'positions_minus_x': False,\n",
      " 'positions_minus_y': True,\n",
      " 'positions_swap_axes': False,\n",
      " 'readout_bias': False,\n",
      " 'readout_gamma': 0.17,\n",
      " 'reg_group_sparsity': 0.1,\n",
      " 'reg_readout_spatial_smoothness': 0.0027,\n",
      " 'reg_spatial_sparsity': 0.45,\n",
      " 'rot_eq_batch_norm': True,\n",
      " 'sample': False,\n",
      " 'scale_init': 0.3,\n",
      " 'seed': 42,\n",
      " 'sigma_x_init': 0.56,\n",
      " 'sigma_y_init': 0.67,\n",
      " 'stack': -1,\n",
      " 'stimulus_crop': None,\n",
      " 'stride': 1,\n",
      " 'test': True,\n",
      " 'test_average_batch': False,\n",
      " 'test_data_dir': '/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/ten_trials.pickle',\n",
      " 'train_data_dir': '/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/one_trials.pickle',\n",
      " 'train_on_test': False,\n",
      " 'train_on_val': False,\n",
      " 'upsampling': 2,\n",
      " 'use_avg_reg': True,\n",
      " 'val_size': 5000}\n",
      "Setting up the dataset...\n",
      "Data loaded successfully!\n",
      "Loaded precomputed mean from /storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/one_trials_mean.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/neuralpredictors/measures/modules.py:78: UserWarning: Poissonloss is averaged per batch. It's recommended to use `sum` instead\n",
      "  warnings.warn(\"Poissonloss is averaged per batch. It's recommended to use `sum` instead\")\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily-field-273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-78fdadac-cb84-e2a1-5ae2-1111d6ed43f1]\n",
      "\n",
      "  | Name   | Type                           | Params\n",
      "----------------------------------------------------------\n",
      "0 | loss   | PoissonLoss                    | 0     \n",
      "1 | corr   | Corr                           | 0     \n",
      "2 | nonlin | PiecewiseLinearExpNonlinearity | 603   \n",
      "----------------------------------------------------------\n",
      "307       Trainable params\n",
      "302       Non-trainable params\n",
      "609       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:452: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "min = 0.001385419280268252, max = 1472.9520263671875\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  2.68it/s]0.07180874\n",
      "0.07180874\n",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  22%|██▏       | 998/4500 [00:34<02:00, 28.95it/s, loss=0.176, v_num=5zgb]\n",
      "min = 5.964456249785144e-06, max = 135.7107391357422\n",
      "Epoch 0:  44%|████▍     | 1998/4500 [01:08<01:25, 29.16it/s, loss=-0.333, v_num=5zgb]   \n",
      "min = 1.1388752682250924e-05, max = 31.94652557373047\n",
      "Epoch 0:  67%|██████▋   | 2998/4500 [01:42<00:51, 29.25it/s, loss=-0.368, v_num=5zgb]\n",
      "min = 9.586748092260677e-06, max = 25.284889221191406\n",
      "Epoch 0:  89%|████████▉ | 3998/4500 [02:16<00:17, 29.28it/s, loss=-0.349, v_num=5zgb]\n",
      "min = 1.143398094427539e-06, max = 12.743884086608887\n",
      "Epoch 0: 100%|█████████▉| 4496/4500 [02:22<00:00, 31.64it/s, loss=-0.328, v_num=5zgb]0.12746374\n",
      "0.12746374\n",
      "Epoch 1:   5%|▌         | 240/4500 [00:08<02:25, 29.27it/s, loss=-0.324, v_num=5zgb] Best model's val/corr: 0.12746374\n",
      "EnergyModel\n",
      "<wandb.sdk.wandb_artifacts.Artifact object at 0x14b5ddd0d8e0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/brno2/home/mpicek/MODEL_CHECKPOINTS/daily-field-273/epoch=0-step=3999.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-78fdadac-cb84-e2a1-5ae2-1111d6ed43f1]\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:452: UserWarning: Your `test_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]\n",
      "min = 8.097172212728765e-06, max = 35.30929183959961\n",
      "Testing: 100%|██████████| 500/500 [00:06<00:00, 82.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-78fdadac-cb84-e2a1-5ae2-1111d6ed43f1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing:  46%|████▌     | 230/500 [00:02<00:02, 92.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  47%|████▋     | 236/500 [00:04<00:04, 57.60it/s]]\n",
      "Testing:  52%|█████▏    | 260/500 [00:02<00:01, 125.45it/s]\n",
      "min = 1.0244353688904084e-05, max = 13.846046447753906\n",
      "Testing:  65%|██████▌   | 325/500 [00:02<00:01, 125.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 152, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 125, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 388, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 213, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 218, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▊| 493/500 [00:04<00:00, 124.82it/s]"
     ]
    }
   ],
   "source": [
    "model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
