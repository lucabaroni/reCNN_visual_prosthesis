{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from model_trainer import run_wandb_training, run_training_without_logging\n",
    "from energy_model.energy_model import EnergyModel\n",
    "from model_trainer import Antolik_dataset_preparation_function\n",
    "from utils import get_config\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ENTITY = \"csng-cuni\"\n",
    "PROJECT = \"reCNN_visual_prosthesis\"\n",
    "model = None\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "train_on_test = False\n",
    "max_epochs = 1\n",
    "max_time = 3\n",
    "patience = 7\n",
    "train_on_val = False\n",
    "test = True\n",
    "seed = 42\n",
    "batch_size = 10\n",
    "lr = 0.001\n",
    "\n",
    "def main():\n",
    "\n",
    "    config = get_config(model=\"EM\")\n",
    "\n",
    "    # TRAINING PARAMETERS\n",
    "    config[\"train_on_test\"] = train_on_test\n",
    "    config[\"max_epochs\"] = max_epochs\n",
    "    config[\"max_time\"] = max_time\n",
    "    config[\"patience\"] = patience\n",
    "    config[\"train_on_val\"] = train_on_val\n",
    "    config[\"test\"] = test\n",
    "    config[\"seed\"] = seed\n",
    "    config[\"batch_size\"] = batch_size\n",
    "    config[\"lr\"] = lr\n",
    "    \n",
    "    config[\"train_data_dir\"] = \"/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/one_trials.pickle\"\n",
    "    config[\"test_data_dir\"] = \"/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/ten_trials.pickle\"\n",
    "\n",
    "    model = run_wandb_training(\n",
    "        config,\n",
    "        Antolik_dataset_preparation_function,\n",
    "        ENTITY,\n",
    "        PROJECT,\n",
    "        model_class=EnergyModel\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2tvark34) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 614918... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>test/corr</td><td>█▁</td></tr><tr><td>test/fraction_oracle_conservative</td><td>▁</td></tr><tr><td>test/fraction_oracle_jackknife</td><td>▁</td></tr><tr><td>test/repeated_trials/corr</td><td>▁</td></tr><tr><td>train/loss</td><td>█▃▁▃</td></tr><tr><td>train/smoothness_penalty</td><td>██▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▃▆█████</td></tr><tr><td>val/corr</td><td>▁</td></tr><tr><td>val/f</td><td>▁▆▇██</td></tr><tr><td>val/loss</td><td>█▃▁▃▁</td></tr><tr><td>val/sigma_x</td><td>█▄▂▁▁</td></tr><tr><td>val/sigma_y</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>test/corr</td><td>0.12315</td></tr><tr><td>test/fraction_oracle_conservative</td><td>0.33492</td></tr><tr><td>test/fraction_oracle_jackknife</td><td>0.41498</td></tr><tr><td>test/repeated_trials/corr</td><td>0.19684</td></tr><tr><td>train/loss</td><td>-0.16233</td></tr><tr><td>train/smoothness_penalty</td><td>0.0011</td></tr><tr><td>trainer/global_step</td><td>4000</td></tr><tr><td>val/corr</td><td>0.12754</td></tr><tr><td>val/f</td><td>0.92619</td></tr><tr><td>val/loss</td><td>-0.3523</td></tr><tr><td>val/sigma_x</td><td>0.42443</td></tr><tr><td>val/sigma_y</td><td>0.0569</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">light-planet-435</strong>: <a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/2tvark34\" target=\"_blank\">https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/2tvark34</a><br/>\n",
       "Find logs at: <code>./wandb/run-20231102_101139-2tvark34/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2tvark34). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/1j5ntd02\" target=\"_blank\">major-tree-438</a></strong> to <a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 10,\n",
      " 'bias_init': 2.5,\n",
      " 'bottleneck_kernel': 15,\n",
      " 'brain_crop': None,\n",
      " 'compute_oracle_fraction': False,\n",
      " 'conservative_oracle': True,\n",
      " 'core_gamma_hidden': 0.28463619129195233,\n",
      " 'core_gamma_input': 0.00307424496692959,\n",
      " 'core_hidden_channels': 3,\n",
      " 'core_hidden_kern': 3,\n",
      " 'core_input_kern': 3,\n",
      " 'core_layers': 1,\n",
      " 'counter_clockwise_rotation': True,\n",
      " 'dataset_artifact_name': 'Antolik_dataset:latest',\n",
      " 'default_ori_shift': 90,\n",
      " 'depth_separable': True,\n",
      " 'do_not_sample': True,\n",
      " 'em_bias': True,\n",
      " 'exact_init': True,\n",
      " 'f_init': 0.63,\n",
      " 'factor': 5.5,\n",
      " 'fixed_sigma': False,\n",
      " 'freeze_orientations': False,\n",
      " 'freeze_positions': False,\n",
      " 'generate_oracle_figure': False,\n",
      " 'ground_truth_orientations_file_path': 'data/antolik/oris_reparametrized.pickle',\n",
      " 'ground_truth_positions_file_path': 'data/antolik/positions_reparametrized.pickle',\n",
      " 'init_mu_range': 0.3,\n",
      " 'init_sigma_range': 0.1,\n",
      " 'init_to_ground_truth_orientations': True,\n",
      " 'init_to_ground_truth_positions': True,\n",
      " 'input_regularizer': 'LaplaceL2norm',\n",
      " 'jackknife_oracle': True,\n",
      " 'lr': 0.001,\n",
      " 'max_epochs': 1,\n",
      " 'max_time': 3,\n",
      " 'model_needs_dataloader': False,\n",
      " 'multivariate': True,\n",
      " 'needs_ground_truth': True,\n",
      " 'nonlinearity': 'softplus',\n",
      " 'normalize': True,\n",
      " 'num_bins': 100,\n",
      " 'num_rotations': 4,\n",
      " 'observed_val_metric': 'val/corr',\n",
      " 'orientation_shift': 87.4,\n",
      " 'patience': 7,\n",
      " 'positions_minus_x': False,\n",
      " 'positions_minus_y': True,\n",
      " 'positions_swap_axes': False,\n",
      " 'readout_bias': False,\n",
      " 'readout_gamma': 0.17,\n",
      " 'reg_group_sparsity': 0.1,\n",
      " 'reg_readout_spatial_smoothness': 0.0027,\n",
      " 'reg_spatial_sparsity': 0.45,\n",
      " 'rot_eq_batch_norm': True,\n",
      " 'sample': False,\n",
      " 'scale_init': 0.3,\n",
      " 'seed': 42,\n",
      " 'sigma_x_init': 0.56,\n",
      " 'sigma_y_init': 0.67,\n",
      " 'smooth_reg_weight': 0.0014451681045518333,\n",
      " 'smoothness_reg_order': 3,\n",
      " 'stack': -1,\n",
      " 'stimulus_crop': None,\n",
      " 'stride': 1,\n",
      " 'test': True,\n",
      " 'test_average_batch': False,\n",
      " 'test_data_dir': '/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/ten_trials.pickle',\n",
      " 'train_data_dir': '/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/one_trials.pickle',\n",
      " 'train_on_test': False,\n",
      " 'train_on_val': False,\n",
      " 'upsampling': 2,\n",
      " 'use_avg_reg': True,\n",
      " 'val_size': 5000,\n",
      " 'vmax': 100}\n",
      "Setting up the dataset...\n",
      "Data loaded successfully!\n",
      "Loaded precomputed mean from /storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized_small/one_trials_mean.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/neuralpredictors/measures/modules.py:78: UserWarning: Poissonloss is averaged per batch. It's recommended to use `sum` instead\n",
      "  warnings.warn(\"Poissonloss is averaged per batch. It's recommended to use `sum` instead\")\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-1206a2bf-1af2-d74d-dc6e-2897ae866310]\n",
      "\n",
      "  | Name   | Type                           | Params\n",
      "----------------------------------------------------------\n",
      "0 | loss   | PoissonLoss                    | 0     \n",
      "1 | corr   | Corr                           | 0     \n",
      "2 | nonlin | PiecewiseLinearExpNonlinearity | 203   \n",
      "----------------------------------------------------------\n",
      "104       Trainable params\n",
      "102       Non-trainable params\n",
      "206       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major-tree-438\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]0.07180692\n",
      "0.07180692\n",
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:452: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 42\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 4495/4500 [02:51<00:00, 26.18it/s, loss=-0.327, v_num=td02]  0.1275436\n",
      "0.1275436\n",
      "Epoch 1: 100%|██████████| 4500/4500 [02:50<00:00, 26.32it/s, loss=-0.364, v_num=td02]0.13718034\n",
      "0.13718034\n",
      "Epoch 2: 100%|██████████| 4500/4500 [02:51<00:00, 26.30it/s, loss=-0.39, v_num=td02] 0.14885584\n",
      "0.14885584\n",
      "Epoch 3: 100%|██████████| 4500/4500 [02:51<00:00, 26.31it/s, loss=-0.391, v_num=td02]0.1580606\n",
      "0.1580606\n",
      "Epoch 4: 100%|██████████| 4500/4500 [02:51<00:00, 26.25it/s, loss=-0.383, v_num=td02]0.16033399\n",
      "0.16033399\n",
      "Epoch 5: 100%|██████████| 4500/4500 [02:50<00:00, 26.32it/s, loss=-0.37, v_num=td02] 0.16088304\n",
      "0.16088304\n",
      "Epoch 6: 100%|██████████| 4500/4500 [02:51<00:00, 26.28it/s, loss=-0.392, v_num=td02]0.16149531\n",
      "0.16149531\n",
      "Epoch 7: 100%|██████████| 4500/4500 [02:51<00:00, 26.30it/s, loss=-0.37, v_num=td02] 0.16251938\n",
      "0.16251938\n",
      "Epoch 8: 100%|██████████| 4500/4500 [02:51<00:00, 26.29it/s, loss=-0.306, v_num=td02]0.16284852\n",
      "0.16284852\n",
      "Epoch 9: 100%|██████████| 4500/4500 [02:51<00:00, 26.30it/s, loss=-0.328, v_num=td02]0.16322675\n",
      "0.16322675\n",
      "Epoch 10: 100%|██████████| 4500/4500 [02:51<00:00, 26.27it/s, loss=-0.35, v_num=td02] 0.16421245\n",
      "0.16421245\n",
      "Epoch 11: 100%|██████████| 4500/4500 [02:51<00:00, 26.28it/s, loss=-0.366, v_num=td02]0.16429397\n",
      "0.16429397\n",
      "Epoch 12: 100%|██████████| 4500/4500 [02:50<00:00, 26.32it/s, loss=-0.35, v_num=td02] 0.16438352\n",
      "0.16438352\n",
      "Epoch 13: 100%|██████████| 4500/4500 [02:51<00:00, 26.28it/s, loss=-0.337, v_num=td02]0.1641807\n",
      "0.1641807\n",
      "Epoch 14: 100%|██████████| 4500/4500 [02:50<00:00, 26.37it/s, loss=-0.37, v_num=td02] 0.16435347\n",
      "0.16435347\n",
      "Epoch 15: 100%|██████████| 4500/4500 [02:50<00:00, 26.39it/s, loss=-0.388, v_num=td02]0.16442063\n",
      "0.16442063\n",
      "Epoch 16: 100%|██████████| 4500/4500 [02:49<00:00, 26.50it/s, loss=-0.399, v_num=td02]0.16435812\n",
      "0.16435812\n",
      "Epoch 17: 100%|██████████| 4500/4500 [02:49<00:00, 26.51it/s, loss=-0.385, v_num=td02]0.1643635\n",
      "0.1643635\n",
      "Epoch 18: 100%|██████████| 4500/4500 [02:49<00:00, 26.54it/s, loss=-0.412, v_num=td02]0.16439839\n",
      "0.16439839\n",
      "Epoch 19: 100%|██████████| 4500/4500 [02:49<00:00, 26.55it/s, loss=-0.375, v_num=td02]0.16435502\n",
      "0.16435502\n",
      "Epoch 20: 100%|██████████| 4500/4500 [02:49<00:00, 26.59it/s, loss=-0.338, v_num=td02]0.16448106\n",
      "0.16448106\n",
      "Epoch 21: 100%|██████████| 4500/4500 [02:49<00:00, 26.60it/s, loss=-0.362, v_num=td02]0.16452152\n",
      "0.16452152\n",
      "Epoch 22: 100%|██████████| 4500/4500 [02:49<00:00, 26.60it/s, loss=-0.374, v_num=td02]0.16455285\n",
      "0.16455285\n",
      "Epoch 23: 100%|██████████| 4500/4500 [02:48<00:00, 26.64it/s, loss=-0.35, v_num=td02] 0.16447674\n",
      "0.16447674\n",
      "Epoch 24: 100%|██████████| 4500/4500 [02:48<00:00, 26.63it/s, loss=-0.399, v_num=td02]0.1644087\n",
      "0.1644087\n",
      "Epoch 25: 100%|██████████| 4500/4500 [02:48<00:00, 26.68it/s, loss=-0.333, v_num=td02]0.1643114\n",
      "0.1643114\n",
      "Epoch 26: 100%|██████████| 4500/4500 [02:48<00:00, 26.71it/s, loss=-0.337, v_num=td02]0.16454183\n",
      "0.16454183\n",
      "Epoch 27: 100%|██████████| 4500/4500 [02:48<00:00, 26.71it/s, loss=-0.324, v_num=td02]0.16456263\n",
      "0.16456263\n",
      "Epoch 28: 100%|██████████| 4500/4500 [02:48<00:00, 26.68it/s, loss=-0.356, v_num=td02]0.1644403\n",
      "0.1644403\n",
      "Epoch 29: 100%|██████████| 4500/4500 [02:48<00:00, 26.71it/s, loss=-0.359, v_num=td02]0.16449052\n",
      "0.16449052\n",
      "Epoch 30: 100%|██████████| 4500/4500 [02:48<00:00, 26.69it/s, loss=-0.354, v_num=td02]0.16447248\n",
      "0.16447248\n",
      "Epoch 31: 100%|██████████| 4500/4500 [02:48<00:00, 26.63it/s, loss=-0.363, v_num=td02]0.16443159\n",
      "0.16443159\n",
      "Epoch 32: 100%|██████████| 4500/4500 [02:48<00:00, 26.65it/s, loss=-0.412, v_num=td02]0.16434775\n",
      "0.16434775\n",
      "Epoch 33: 100%|██████████| 4500/4500 [02:48<00:00, 26.66it/s, loss=-0.406, v_num=td02]0.1645429\n",
      "0.1645429\n",
      "Epoch 34: 100%|██████████| 4500/4500 [02:48<00:00, 26.67it/s, loss=-0.358, v_num=td02]0.16447507\n",
      "0.16447507\n",
      "Epoch 34: 100%|██████████| 4500/4500 [02:49<00:00, 26.57it/s, loss=-0.358, v_num=td02]\n",
      "Best model's val/corr: 0.16456263\n",
      "EnergyModel\n",
      "<wandb.sdk.wandb_artifacts.Artifact object at 0x14c8f5a09250>\n",
      "/storage/brno2/home/mpicek/MODEL_CHECKPOINTS/major-tree-438/epoch=27-step=111999.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-1206a2bf-1af2-d74d-dc6e-2897ae866310]\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:452: UserWarning: Your `test_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 500/500 [00:07<00:00, 66.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-1206a2bf-1af2-d74d-dc6e-2897ae866310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 100%|██████████| 500/500 [00:07<00:00, 66.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-1206a2bf-1af2-d74d-dc6e-2897ae866310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 100%|██████████| 500/500 [00:06<00:00, 72.30it/s]\n",
      "Validation dataset:\n",
      "    Correlation: 0.1646 \n",
      "Test dataset with averaged responses of repeated trials:\n",
      "    Correlation: 0.2504 \n",
      "    Fraction oracle conservative: 0.4227 \n",
      "    Fraction oracle jackknife: 0.5206 \n"
     ]
    }
   ],
   "source": [
    "model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
