<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Antolik_dataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Antolik_dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pickle
import pytorch_lightning as pl
import numpy as np
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
import torch
from neuralpredictors.data.samplers import SubsetSequentialSampler
from typing import Optional
import pathlib
from torch.utils.data import Dataset
from torchvision import transforms


class AntolikDataset(Dataset):
    &#34;&#34;&#34;A class for handling with the Antolik&#39;s synthetic dataset.&#34;&#34;&#34;    

    def __init__(self, path, normalize=True):
        &#34;&#34;&#34;The constructor.

        Args:
            path (str): Path to the dataset
            normalize (bool, optional): Whether to normalize the images. Defaults to True.
        &#34;&#34;&#34;
        self.normalize = normalize

        self.data = self.pickle_read(path)

        self.transform_list = transforms.Compose(
            [transforms.Normalize((45.2315,), (26.6845,))]
        )

    def __getitem__(self, index):
        &#34;&#34;&#34;Gets the index-th pair of visual stimulus and response to the stimulus.

        Args:
            index (int): the index

        Returns:
            (np.array, np.array): The index-th pair of visual stimulus and response to the stimulus.
        &#34;&#34;&#34;
        x = self.data[index][&#34;stimulus&#34;]
        x = np.expand_dims(x, axis=0)
        y = np.concatenate(
            [self.data[index][&#34;V1_Exc_L2/3&#34;], self.data[index][&#34;V1_Inh_L2/3&#34;]]
        )

        data = torch.from_numpy(x)
        target = torch.from_numpy(y)

        if self.normalize:
            data = self.transform_list(data)

        return (data.float(), target.float())

    def __len__(self):
        &#34;&#34;&#34;

        Returns:
            int: The length of the dataset
        &#34;&#34;&#34;
        return len(self.data)

    def pickle_read(self, path):
        &#34;&#34;&#34;A helper function to unpickle the dataset.

        Args:
            path (str): Path to the dataset

        Returns:
            The dataset.
        &#34;&#34;&#34;
        with open(path, &#34;rb&#34;) as f:
            x = pickle.load(f)
        return x


class AntolikDataModule(pl.LightningDataModule):
    &#34;&#34;&#34;
    Warning! Has to be downloaded from the wintermute server!
    A Pytorch Lightning module that uses Antolik&#39;s dataset.
    &#34;&#34;&#34;

    def __init__(
        self,
        train_data_dir,
        test_data_dir,
        batch_size,
        # seed=None,
        normalize=True,
        num_workers=0,
        val_size=5000,
    ):
        &#34;&#34;&#34;The constructor.

        Args:
            train_data_dir (str): Path to the train dataset
            test_data_dir (str): Path to the test dataset
            batch_size (int): Batch size
            normalize (bool, optional): Whether to normalize the input images. Defaults to True.
            num_workers (int, optional): Number of workers that load the dataset. Defaults to 0.
            val_size (int, optional): Validation dataset length. Defaults to 5000.
        &#34;&#34;&#34;
        super().__init__()
        self.train_data_dir = train_data_dir
        self.test_data_dir = test_data_dir
        self.batch_size = batch_size
        # seed=None,
        self.normalize = normalize
        self.num_workers = num_workers
        self.val_size = val_size

    def prepare_data(self):
        &#34;&#34;&#34;We do not have public access to the data. This function will be implemented
        when the dataset is available on some website.

        Raises:
            Exception: The train .pickle file does not exist
            Exception: The test .pickle file does not exist
        &#34;&#34;&#34;        

        # we should not do anything like self.x = y # = assign state
        # just download the data
        train_path = pathlib.Path(self.train_data_dir)
        test_path = pathlib.Path(self.test_data_dir)

        if not train_path.exists():
            raise Exception(
                &#34;The .pickle file with Antolik train dataset does not exist.&#34;
            )

        if not test_path.exists():
            raise Exception(
                &#34;The .pickle file with Antolik test dataset does not exist.&#34;
            )

    def setup(self, stage: Optional[str] = None):
        &#34;&#34;&#34;Sets up the dataset, loads it, shuffles it and sets up the sampler.

        Args:
            stage (Optional[str], optional): Possible values are &#39;fit, &#39;test&#39;,
            &#39;predict&#39; or None. None means both &#39;fit&#39; and &#39;test&#39;. If &#39;fit&#39;, the
            method sets up only the train dataset. If &#39;test&#39;, it sets up only
            the train dataset. If &#39;predict&#39;, it sets up the train dataset.
            Defaults to None.
        &#34;&#34;&#34;
        # stage is &#34;fit&#34; or &#34;test&#34; or &#34;predict&#34;
        # when stage=None -&gt; both &#34;fit&#34; and &#34;test&#34;

        self.train_dataset = AntolikDataset(
            self.train_data_dir, normalize=self.normalize
        )
        self.train_data = self.pickle_read(self.train_data_dir)
        self.test_dataset = AntolikDataset(self.test_data_dir, normalize=self.normalize)
        self.test_data = self.pickle_read(self.test_data_dir)

        print(&#34;Data loaded successfully!&#34;)

        # Assign train/val datasets for use in dataloaders
        if stage == &#34;fit&#34; or stage == &#34;predict&#34; or stage is None:

            indices = np.arange(0, len(self.train_data))

            rng = np.random.default_rng(69)
            rng.shuffle(indices)
            indices_keys = [list(self.train_data.keys())[i] for i in indices]

            subset_idx_val = indices_keys[0 : self.val_size]
            subset_idx_train = indices_keys[self.val_size :]

            self.subset_idx_val = subset_idx_val

            self.train_random_sampler = SubsetRandomSampler(subset_idx_train)
            self.train_sequential_sampler = SubsetSequentialSampler(subset_idx_train)
            self.val_sampler = SubsetSequentialSampler(subset_idx_val)

        if stage == &#34;test&#34; or stage is None:
            indices = np.arange(0, len(self.test_data))
            subset_idx_test = [list(self.test_data.keys())[i] for i in indices]
            self.test_sampler = SubsetSequentialSampler(subset_idx_test)

    def get_input_shape(self):
        x, _ = next(iter(self.train_dataloader()))
        return x[0].shape

    def get_output_shape(self):
        _, y = next(iter(self.train_dataloader()))
        return y[0].shape

    def get_mean(self):
        &#34;&#34;&#34;Computes the mean response of the train dataset. If it is available
        in a locally generated file, it loads it from there. Otherwise it
        computes it and then it stores it into the file for the future use.

        Returns:
            torch.Tensor: Mean responses of the neurons.
        &#34;&#34;&#34;        

        mean_path = pathlib.Path(
            self.train_data_dir.rsplit(&#34;.pickle&#34;, 1)[0] + &#34;_mean.npy&#34;
        )

        if mean_path.exists():
            mean = np.load(mean_path)
            print(&#34;Loaded precomputed mean from &#34; + str(mean_path))
            return torch.from_numpy(mean)

        dataloader = DataLoader(
            self.train_dataset,
            sampler=self.train_sequential_sampler,
            batch_size=self.batch_size,
        )
        summed = torch.zeros(self.get_output_shape())

        for (_, y) in dataloader:
            summed += torch.sum(y, 0)

        mean = summed / self.train_len()

        np.save(mean_path, mean)
        print(&#34;Created mean array and saved to &#34; + str(mean_path))
        return mean

    def train_len(self):
        return len(self.train_random_sampler)

    def val_len(self):
        return len(self.val_sampler)

    def test_len(self):
        return len(self.test_sampler)

    def __len__(self):
        &#34;&#34;&#34;The length of ALL the data we have (train + val + test)

        Returns:
            int: The length of ALL the data we have (train + val + test)
        &#34;&#34;&#34;        
        return self.train_len() + self.val_len() + self.test_len()

    def print_dataset_info(self):
        &#34;&#34;&#34;Creates a train dataloader, gets first piece of data and prints its shape
        &#34;&#34;&#34;
        print(&#34; ------------ DATASET INFO ------------ &#34;)
        print(&#34; SHAPES:&#34;)
        dataloader = DataLoader(
            self.train_dataset,
            sampler=self.train_random_sampler,
            batch_size=self.batch_size,
        )
        print(f&#34;    Input shape (images): {self.get_input_shape()}&#34;)
        print(&#34;    With batch size also: &#34;, end=&#34;&#34;)
        print(next(iter(dataloader))[0].shape)

        print(f&#34;    Output shape (responses): {self.get_output_shape()}&#34;)
        print(&#34;    With batch size also: &#34;, end=&#34;&#34;)
        print(next(iter(dataloader))[1].shape)

        print(&#34; LENGTH:&#34;)
        print(f&#34;    Length of the dataset is {len(self)}&#34;)
        print(f&#34;    Length of the train set is {self.train_len()}&#34;)
        print(f&#34;    Length of the val set is {self.val_len()}&#34;)
        print(f&#34;    Length of the test set is {self.test_len()}&#34;)

        print(&#34; -------------------------------------- &#34;)

    def train_dataloader(self):
        &#34;&#34;&#34;
        Returns:
            DataLoader: The train DataLoader
        &#34;&#34;&#34;        
        return DataLoader(
            self.train_dataset,
            sampler=self.train_random_sampler,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def val_dataloader(self):
        &#34;&#34;&#34;
        Validation data are in the variable train_data 
        (but the indices are splitted to self.val_sampler)

        Returns:
            DataLoader: The validation DataLoader
        &#34;&#34;&#34;   
        return DataLoader(
            self.train_dataset,
            sampler=self.val_sampler,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def test_dataloader(self):
        &#34;&#34;&#34;
        Returns:
            DataLoader: The test DataLoader
        &#34;&#34;&#34;   
        return DataLoader(
            self.test_dataset,
            sampler=self.test_sampler,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def get_oracle_dataloader(self):
        &#34;&#34;&#34;the only difference from test_dataloader is that we hardcode batch_size=10

        Returns:
            DataLoader: The test DataLoader with batch_size=10
        &#34;&#34;&#34;
        
        return DataLoader(
            self.test_dataset,
            sampler=self.test_sampler,
            batch_size=10,
            num_workers=self.num_workers,
        )

    def model_performances(self, model=None, trainer=None, control_measures=None):
        &#34;&#34;&#34;Evaluates the model and prints the results

        Args:
            model (pl.model, optional): Model to be evaluated. Defaults to None.
            trainer (pl.trainer, optional): The trainer that performs the evaluation. Defaults to None.
            control_measures (dict, optional): The control model&#39;s measures to be compared with our evaluated model. Defaults to None.

        Returns:
            dict: Dictionary of the resulting measures.
        &#34;&#34;&#34;
        model.test_average_batch = False
        model.compute_oracle_fraction = False
        val_score = trainer.test(model, self.val_dataloader(), verbose=False)
        test_score = trainer.test(model, self.test_dataloader(), verbose=False)

        model.test_average_batch = True
        model.compute_oracle_fraction = True
        test_repeats_averaged_score = trainer.test(model, self.get_oracle_dataloader(), verbose=False)

        val_score = val_score[0]
        test_score = test_score[0]
        test_repeats_averaged_score = test_repeats_averaged_score[0]

        print(&#34;Validation dataset:&#34;)
        print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(val_score[&#39;test/corr&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (val_score[&#39;test/corr&#39;] / control_measures[&#39;val/corr&#39;])) if control_measures else &#39;&#39;}&#34;)


        # print(&#34;Test dataset:&#34;)
        # print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(test_score[&#39;test/corr&#39;]) }&#34;)

        print(&#34;Test dataset with averaged responses of repeated trials:&#34;)
        print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;]) } {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;] / control_measures[&#39;test/repeated_trials/corr&#39;])) if control_measures else &#39;&#39;}&#34;)
        print(f&#34;    Fraction oracle conservative: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;] / control_measures[&#39;test/fraction_oracle_conservative&#39;])) if control_measures else &#39;&#39;}&#34;)
        print(f&#34;    Fraction oracle jackknife: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;] / control_measures[&#39;test/fraction_oracle_jackknife&#39;])) if control_measures else &#39;&#39;}&#34;)

        returned_measures = {
            &#34;val/corr&#34;: val_score[&#39;test/corr&#39;],
            &#34;test/repeated_trials/corr&#34;: test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;],
            &#34;test/fraction_oracle_conservative&#34;:test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;],
            &#34;test/fraction_oracle_jackknife&#34;:test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;]
        }

        return returned_measures


    def pickle_read(self, path):
        with open(path, &#34;rb&#34;) as f:
            x = pickle.load(f)
        return x


if __name__ == &#34;__main__&#34;:

    path_train = &#34;/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik/one_trials.pickle&#34;
    path_test = &#34;/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik/ten_trials.pickle&#34;

    path_small_train = &#34;/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik/small_train.pickle&#34;

    dm = AntolikDataModule(path_test, path_test, 10, val_size=500)
    dm.prepare_data()
    dm.setup()
    dm.print_dataset_info()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Antolik_dataset.AntolikDataModule"><code class="flex name class">
<span>class <span class="ident">AntolikDataModule</span></span>
<span>(</span><span>train_data_dir, test_data_dir, batch_size, normalize=True, num_workers=0, val_size=5000)</span>
</code></dt>
<dd>
<div class="desc"><p>Warning! Has to be downloaded from the wintermute server!
A Pytorch Lightning module that uses Antolik's dataset.</p>
<p>The constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the train dataset</dd>
<dt><strong><code>test_data_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the test dataset</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch size</dd>
<dt><strong><code>normalize</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to normalize the input images. Defaults to True.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of workers that load the dataset. Defaults to 0.</dd>
<dt><strong><code>val_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Validation dataset length. Defaults to 5000.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AntolikDataModule(pl.LightningDataModule):
    &#34;&#34;&#34;
    Warning! Has to be downloaded from the wintermute server!
    A Pytorch Lightning module that uses Antolik&#39;s dataset.
    &#34;&#34;&#34;

    def __init__(
        self,
        train_data_dir,
        test_data_dir,
        batch_size,
        # seed=None,
        normalize=True,
        num_workers=0,
        val_size=5000,
    ):
        &#34;&#34;&#34;The constructor.

        Args:
            train_data_dir (str): Path to the train dataset
            test_data_dir (str): Path to the test dataset
            batch_size (int): Batch size
            normalize (bool, optional): Whether to normalize the input images. Defaults to True.
            num_workers (int, optional): Number of workers that load the dataset. Defaults to 0.
            val_size (int, optional): Validation dataset length. Defaults to 5000.
        &#34;&#34;&#34;
        super().__init__()
        self.train_data_dir = train_data_dir
        self.test_data_dir = test_data_dir
        self.batch_size = batch_size
        # seed=None,
        self.normalize = normalize
        self.num_workers = num_workers
        self.val_size = val_size

    def prepare_data(self):
        &#34;&#34;&#34;We do not have public access to the data. This function will be implemented
        when the dataset is available on some website.

        Raises:
            Exception: The train .pickle file does not exist
            Exception: The test .pickle file does not exist
        &#34;&#34;&#34;        

        # we should not do anything like self.x = y # = assign state
        # just download the data
        train_path = pathlib.Path(self.train_data_dir)
        test_path = pathlib.Path(self.test_data_dir)

        if not train_path.exists():
            raise Exception(
                &#34;The .pickle file with Antolik train dataset does not exist.&#34;
            )

        if not test_path.exists():
            raise Exception(
                &#34;The .pickle file with Antolik test dataset does not exist.&#34;
            )

    def setup(self, stage: Optional[str] = None):
        &#34;&#34;&#34;Sets up the dataset, loads it, shuffles it and sets up the sampler.

        Args:
            stage (Optional[str], optional): Possible values are &#39;fit, &#39;test&#39;,
            &#39;predict&#39; or None. None means both &#39;fit&#39; and &#39;test&#39;. If &#39;fit&#39;, the
            method sets up only the train dataset. If &#39;test&#39;, it sets up only
            the train dataset. If &#39;predict&#39;, it sets up the train dataset.
            Defaults to None.
        &#34;&#34;&#34;
        # stage is &#34;fit&#34; or &#34;test&#34; or &#34;predict&#34;
        # when stage=None -&gt; both &#34;fit&#34; and &#34;test&#34;

        self.train_dataset = AntolikDataset(
            self.train_data_dir, normalize=self.normalize
        )
        self.train_data = self.pickle_read(self.train_data_dir)
        self.test_dataset = AntolikDataset(self.test_data_dir, normalize=self.normalize)
        self.test_data = self.pickle_read(self.test_data_dir)

        print(&#34;Data loaded successfully!&#34;)

        # Assign train/val datasets for use in dataloaders
        if stage == &#34;fit&#34; or stage == &#34;predict&#34; or stage is None:

            indices = np.arange(0, len(self.train_data))

            rng = np.random.default_rng(69)
            rng.shuffle(indices)
            indices_keys = [list(self.train_data.keys())[i] for i in indices]

            subset_idx_val = indices_keys[0 : self.val_size]
            subset_idx_train = indices_keys[self.val_size :]

            self.subset_idx_val = subset_idx_val

            self.train_random_sampler = SubsetRandomSampler(subset_idx_train)
            self.train_sequential_sampler = SubsetSequentialSampler(subset_idx_train)
            self.val_sampler = SubsetSequentialSampler(subset_idx_val)

        if stage == &#34;test&#34; or stage is None:
            indices = np.arange(0, len(self.test_data))
            subset_idx_test = [list(self.test_data.keys())[i] for i in indices]
            self.test_sampler = SubsetSequentialSampler(subset_idx_test)

    def get_input_shape(self):
        x, _ = next(iter(self.train_dataloader()))
        return x[0].shape

    def get_output_shape(self):
        _, y = next(iter(self.train_dataloader()))
        return y[0].shape

    def get_mean(self):
        &#34;&#34;&#34;Computes the mean response of the train dataset. If it is available
        in a locally generated file, it loads it from there. Otherwise it
        computes it and then it stores it into the file for the future use.

        Returns:
            torch.Tensor: Mean responses of the neurons.
        &#34;&#34;&#34;        

        mean_path = pathlib.Path(
            self.train_data_dir.rsplit(&#34;.pickle&#34;, 1)[0] + &#34;_mean.npy&#34;
        )

        if mean_path.exists():
            mean = np.load(mean_path)
            print(&#34;Loaded precomputed mean from &#34; + str(mean_path))
            return torch.from_numpy(mean)

        dataloader = DataLoader(
            self.train_dataset,
            sampler=self.train_sequential_sampler,
            batch_size=self.batch_size,
        )
        summed = torch.zeros(self.get_output_shape())

        for (_, y) in dataloader:
            summed += torch.sum(y, 0)

        mean = summed / self.train_len()

        np.save(mean_path, mean)
        print(&#34;Created mean array and saved to &#34; + str(mean_path))
        return mean

    def train_len(self):
        return len(self.train_random_sampler)

    def val_len(self):
        return len(self.val_sampler)

    def test_len(self):
        return len(self.test_sampler)

    def __len__(self):
        &#34;&#34;&#34;The length of ALL the data we have (train + val + test)

        Returns:
            int: The length of ALL the data we have (train + val + test)
        &#34;&#34;&#34;        
        return self.train_len() + self.val_len() + self.test_len()

    def print_dataset_info(self):
        &#34;&#34;&#34;Creates a train dataloader, gets first piece of data and prints its shape
        &#34;&#34;&#34;
        print(&#34; ------------ DATASET INFO ------------ &#34;)
        print(&#34; SHAPES:&#34;)
        dataloader = DataLoader(
            self.train_dataset,
            sampler=self.train_random_sampler,
            batch_size=self.batch_size,
        )
        print(f&#34;    Input shape (images): {self.get_input_shape()}&#34;)
        print(&#34;    With batch size also: &#34;, end=&#34;&#34;)
        print(next(iter(dataloader))[0].shape)

        print(f&#34;    Output shape (responses): {self.get_output_shape()}&#34;)
        print(&#34;    With batch size also: &#34;, end=&#34;&#34;)
        print(next(iter(dataloader))[1].shape)

        print(&#34; LENGTH:&#34;)
        print(f&#34;    Length of the dataset is {len(self)}&#34;)
        print(f&#34;    Length of the train set is {self.train_len()}&#34;)
        print(f&#34;    Length of the val set is {self.val_len()}&#34;)
        print(f&#34;    Length of the test set is {self.test_len()}&#34;)

        print(&#34; -------------------------------------- &#34;)

    def train_dataloader(self):
        &#34;&#34;&#34;
        Returns:
            DataLoader: The train DataLoader
        &#34;&#34;&#34;        
        return DataLoader(
            self.train_dataset,
            sampler=self.train_random_sampler,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def val_dataloader(self):
        &#34;&#34;&#34;
        Validation data are in the variable train_data 
        (but the indices are splitted to self.val_sampler)

        Returns:
            DataLoader: The validation DataLoader
        &#34;&#34;&#34;   
        return DataLoader(
            self.train_dataset,
            sampler=self.val_sampler,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def test_dataloader(self):
        &#34;&#34;&#34;
        Returns:
            DataLoader: The test DataLoader
        &#34;&#34;&#34;   
        return DataLoader(
            self.test_dataset,
            sampler=self.test_sampler,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
        )

    def get_oracle_dataloader(self):
        &#34;&#34;&#34;the only difference from test_dataloader is that we hardcode batch_size=10

        Returns:
            DataLoader: The test DataLoader with batch_size=10
        &#34;&#34;&#34;
        
        return DataLoader(
            self.test_dataset,
            sampler=self.test_sampler,
            batch_size=10,
            num_workers=self.num_workers,
        )

    def model_performances(self, model=None, trainer=None, control_measures=None):
        &#34;&#34;&#34;Evaluates the model and prints the results

        Args:
            model (pl.model, optional): Model to be evaluated. Defaults to None.
            trainer (pl.trainer, optional): The trainer that performs the evaluation. Defaults to None.
            control_measures (dict, optional): The control model&#39;s measures to be compared with our evaluated model. Defaults to None.

        Returns:
            dict: Dictionary of the resulting measures.
        &#34;&#34;&#34;
        model.test_average_batch = False
        model.compute_oracle_fraction = False
        val_score = trainer.test(model, self.val_dataloader(), verbose=False)
        test_score = trainer.test(model, self.test_dataloader(), verbose=False)

        model.test_average_batch = True
        model.compute_oracle_fraction = True
        test_repeats_averaged_score = trainer.test(model, self.get_oracle_dataloader(), verbose=False)

        val_score = val_score[0]
        test_score = test_score[0]
        test_repeats_averaged_score = test_repeats_averaged_score[0]

        print(&#34;Validation dataset:&#34;)
        print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(val_score[&#39;test/corr&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (val_score[&#39;test/corr&#39;] / control_measures[&#39;val/corr&#39;])) if control_measures else &#39;&#39;}&#34;)


        # print(&#34;Test dataset:&#34;)
        # print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(test_score[&#39;test/corr&#39;]) }&#34;)

        print(&#34;Test dataset with averaged responses of repeated trials:&#34;)
        print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;]) } {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;] / control_measures[&#39;test/repeated_trials/corr&#39;])) if control_measures else &#39;&#39;}&#34;)
        print(f&#34;    Fraction oracle conservative: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;] / control_measures[&#39;test/fraction_oracle_conservative&#39;])) if control_measures else &#39;&#39;}&#34;)
        print(f&#34;    Fraction oracle jackknife: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;] / control_measures[&#39;test/fraction_oracle_jackknife&#39;])) if control_measures else &#39;&#39;}&#34;)

        returned_measures = {
            &#34;val/corr&#34;: val_score[&#39;test/corr&#39;],
            &#34;test/repeated_trials/corr&#34;: test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;],
            &#34;test/fraction_oracle_conservative&#34;:test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;],
            &#34;test/fraction_oracle_jackknife&#34;:test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;]
        }

        return returned_measures


    def pickle_read(self, path):
        with open(path, &#34;rb&#34;) as f:
            x = pickle.load(f)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.datamodule.LightningDataModule</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="Antolik_dataset.AntolikDataModule.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="Antolik_dataset.AntolikDataModule.get_input_shape"><code class="name flex">
<span>def <span class="ident">get_input_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_shape(self):
    x, _ = next(iter(self.train_dataloader()))
    return x[0].shape</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.get_mean"><code class="name flex">
<span>def <span class="ident">get_mean</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the mean response of the train dataset. If it is available
in a locally generated file, it loads it from there. Otherwise it
computes it and then it stores it into the file for the future use.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Mean responses of the neurons.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mean(self):
    &#34;&#34;&#34;Computes the mean response of the train dataset. If it is available
    in a locally generated file, it loads it from there. Otherwise it
    computes it and then it stores it into the file for the future use.

    Returns:
        torch.Tensor: Mean responses of the neurons.
    &#34;&#34;&#34;        

    mean_path = pathlib.Path(
        self.train_data_dir.rsplit(&#34;.pickle&#34;, 1)[0] + &#34;_mean.npy&#34;
    )

    if mean_path.exists():
        mean = np.load(mean_path)
        print(&#34;Loaded precomputed mean from &#34; + str(mean_path))
        return torch.from_numpy(mean)

    dataloader = DataLoader(
        self.train_dataset,
        sampler=self.train_sequential_sampler,
        batch_size=self.batch_size,
    )
    summed = torch.zeros(self.get_output_shape())

    for (_, y) in dataloader:
        summed += torch.sum(y, 0)

    mean = summed / self.train_len()

    np.save(mean_path, mean)
    print(&#34;Created mean array and saved to &#34; + str(mean_path))
    return mean</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.get_oracle_dataloader"><code class="name flex">
<span>def <span class="ident">get_oracle_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>the only difference from test_dataloader is that we hardcode batch_size=10</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataLoader</code></dt>
<dd>The test DataLoader with batch_size=10</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_oracle_dataloader(self):
    &#34;&#34;&#34;the only difference from test_dataloader is that we hardcode batch_size=10

    Returns:
        DataLoader: The test DataLoader with batch_size=10
    &#34;&#34;&#34;
    
    return DataLoader(
        self.test_dataset,
        sampler=self.test_sampler,
        batch_size=10,
        num_workers=self.num_workers,
    )</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.get_output_shape"><code class="name flex">
<span>def <span class="ident">get_output_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_output_shape(self):
    _, y = next(iter(self.train_dataloader()))
    return y[0].shape</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.model_performances"><code class="name flex">
<span>def <span class="ident">model_performances</span></span>(<span>self, model=None, trainer=None, control_measures=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the model and prints the results</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pl.model</code>, optional</dt>
<dd>Model to be evaluated. Defaults to None.</dd>
<dt><strong><code>trainer</code></strong> :&ensp;<code>pl.trainer</code>, optional</dt>
<dd>The trainer that performs the evaluation. Defaults to None.</dd>
<dt><strong><code>control_measures</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>The control model's measures to be compared with our evaluated model. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary of the resulting measures.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_performances(self, model=None, trainer=None, control_measures=None):
    &#34;&#34;&#34;Evaluates the model and prints the results

    Args:
        model (pl.model, optional): Model to be evaluated. Defaults to None.
        trainer (pl.trainer, optional): The trainer that performs the evaluation. Defaults to None.
        control_measures (dict, optional): The control model&#39;s measures to be compared with our evaluated model. Defaults to None.

    Returns:
        dict: Dictionary of the resulting measures.
    &#34;&#34;&#34;
    model.test_average_batch = False
    model.compute_oracle_fraction = False
    val_score = trainer.test(model, self.val_dataloader(), verbose=False)
    test_score = trainer.test(model, self.test_dataloader(), verbose=False)

    model.test_average_batch = True
    model.compute_oracle_fraction = True
    test_repeats_averaged_score = trainer.test(model, self.get_oracle_dataloader(), verbose=False)

    val_score = val_score[0]
    test_score = test_score[0]
    test_repeats_averaged_score = test_repeats_averaged_score[0]

    print(&#34;Validation dataset:&#34;)
    print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(val_score[&#39;test/corr&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (val_score[&#39;test/corr&#39;] / control_measures[&#39;val/corr&#39;])) if control_measures else &#39;&#39;}&#34;)


    # print(&#34;Test dataset:&#34;)
    # print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(test_score[&#39;test/corr&#39;]) }&#34;)

    print(&#34;Test dataset with averaged responses of repeated trials:&#34;)
    print(f&#34;    Correlation: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;]) } {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;] / control_measures[&#39;test/repeated_trials/corr&#39;])) if control_measures else &#39;&#39;}&#34;)
    print(f&#34;    Fraction oracle conservative: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;] / control_measures[&#39;test/fraction_oracle_conservative&#39;])) if control_measures else &#39;&#39;}&#34;)
    print(f&#34;    Fraction oracle jackknife: {&#39;{:.4f}&#39;.format(test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;])} {&#39;({:.2f} percent of the control model)&#39;.format(100 * (test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;] / control_measures[&#39;test/fraction_oracle_jackknife&#39;])) if control_measures else &#39;&#39;}&#34;)

    returned_measures = {
        &#34;val/corr&#34;: val_score[&#39;test/corr&#39;],
        &#34;test/repeated_trials/corr&#34;: test_repeats_averaged_score[&#39;test/repeated_trials/corr&#39;],
        &#34;test/fraction_oracle_conservative&#34;:test_repeats_averaged_score[&#39;test/fraction_oracle_conservative&#39;],
        &#34;test/fraction_oracle_jackknife&#34;:test_repeats_averaged_score[&#39;test/fraction_oracle_jackknife&#39;]
    }

    return returned_measures</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.pickle_read"><code class="name flex">
<span>def <span class="ident">pickle_read</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pickle_read(self, path):
    with open(path, &#34;rb&#34;) as f:
        x = pickle.load(f)
    return x</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>We do not have public access to the data. This function will be implemented
when the dataset is available on some website.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>The train .pickle file does not exist</dd>
<dt><code>Exception</code></dt>
<dd>The test .pickle file does not exist</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self):
    &#34;&#34;&#34;We do not have public access to the data. This function will be implemented
    when the dataset is available on some website.

    Raises:
        Exception: The train .pickle file does not exist
        Exception: The test .pickle file does not exist
    &#34;&#34;&#34;        

    # we should not do anything like self.x = y # = assign state
    # just download the data
    train_path = pathlib.Path(self.train_data_dir)
    test_path = pathlib.Path(self.test_data_dir)

    if not train_path.exists():
        raise Exception(
            &#34;The .pickle file with Antolik train dataset does not exist.&#34;
        )

    if not test_path.exists():
        raise Exception(
            &#34;The .pickle file with Antolik test dataset does not exist.&#34;
        )</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.print_dataset_info"><code class="name flex">
<span>def <span class="ident">print_dataset_info</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a train dataloader, gets first piece of data and prints its shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_dataset_info(self):
    &#34;&#34;&#34;Creates a train dataloader, gets first piece of data and prints its shape
    &#34;&#34;&#34;
    print(&#34; ------------ DATASET INFO ------------ &#34;)
    print(&#34; SHAPES:&#34;)
    dataloader = DataLoader(
        self.train_dataset,
        sampler=self.train_random_sampler,
        batch_size=self.batch_size,
    )
    print(f&#34;    Input shape (images): {self.get_input_shape()}&#34;)
    print(&#34;    With batch size also: &#34;, end=&#34;&#34;)
    print(next(iter(dataloader))[0].shape)

    print(f&#34;    Output shape (responses): {self.get_output_shape()}&#34;)
    print(&#34;    With batch size also: &#34;, end=&#34;&#34;)
    print(next(iter(dataloader))[1].shape)

    print(&#34; LENGTH:&#34;)
    print(f&#34;    Length of the dataset is {len(self)}&#34;)
    print(f&#34;    Length of the train set is {self.train_len()}&#34;)
    print(f&#34;    Length of the val set is {self.val_len()}&#34;)
    print(f&#34;    Length of the test set is {self.test_len()}&#34;)

    print(&#34; -------------------------------------- &#34;)</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, stage: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets up the dataset, loads it, shuffles it and sets up the sampler.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>stage</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>Possible values are 'fit, 'test',</dd>
</dl>
<p>'predict' or None. None means both 'fit' and 'test'. If 'fit', the
method sets up only the train dataset. If 'test', it sets up only
the train dataset. If 'predict', it sets up the train dataset.
Defaults to None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, stage: Optional[str] = None):
    &#34;&#34;&#34;Sets up the dataset, loads it, shuffles it and sets up the sampler.

    Args:
        stage (Optional[str], optional): Possible values are &#39;fit, &#39;test&#39;,
        &#39;predict&#39; or None. None means both &#39;fit&#39; and &#39;test&#39;. If &#39;fit&#39;, the
        method sets up only the train dataset. If &#39;test&#39;, it sets up only
        the train dataset. If &#39;predict&#39;, it sets up the train dataset.
        Defaults to None.
    &#34;&#34;&#34;
    # stage is &#34;fit&#34; or &#34;test&#34; or &#34;predict&#34;
    # when stage=None -&gt; both &#34;fit&#34; and &#34;test&#34;

    self.train_dataset = AntolikDataset(
        self.train_data_dir, normalize=self.normalize
    )
    self.train_data = self.pickle_read(self.train_data_dir)
    self.test_dataset = AntolikDataset(self.test_data_dir, normalize=self.normalize)
    self.test_data = self.pickle_read(self.test_data_dir)

    print(&#34;Data loaded successfully!&#34;)

    # Assign train/val datasets for use in dataloaders
    if stage == &#34;fit&#34; or stage == &#34;predict&#34; or stage is None:

        indices = np.arange(0, len(self.train_data))

        rng = np.random.default_rng(69)
        rng.shuffle(indices)
        indices_keys = [list(self.train_data.keys())[i] for i in indices]

        subset_idx_val = indices_keys[0 : self.val_size]
        subset_idx_train = indices_keys[self.val_size :]

        self.subset_idx_val = subset_idx_val

        self.train_random_sampler = SubsetRandomSampler(subset_idx_train)
        self.train_sequential_sampler = SubsetSequentialSampler(subset_idx_train)
        self.val_sampler = SubsetSequentialSampler(subset_idx_val)

    if stage == &#34;test&#34; or stage is None:
        indices = np.arange(0, len(self.test_data))
        subset_idx_test = [list(self.test_data.keys())[i] for i in indices]
        self.test_sampler = SubsetSequentialSampler(subset_idx_test)</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.test_dataloader"><code class="name flex">
<span>def <span class="ident">test_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>DataLoader</code></dt>
<dd>The test DataLoader</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dataloader(self):
    &#34;&#34;&#34;
    Returns:
        DataLoader: The test DataLoader
    &#34;&#34;&#34;   
    return DataLoader(
        self.test_dataset,
        sampler=self.test_sampler,
        batch_size=self.batch_size,
        num_workers=self.num_workers,
    )</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.test_len"><code class="name flex">
<span>def <span class="ident">test_len</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_len(self):
    return len(self.test_sampler)</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<dl>
<dt><code>DataLoader</code></dt>
<dd>The train DataLoader</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    &#34;&#34;&#34;
    Returns:
        DataLoader: The train DataLoader
    &#34;&#34;&#34;        
    return DataLoader(
        self.train_dataset,
        sampler=self.train_random_sampler,
        batch_size=self.batch_size,
        num_workers=self.num_workers,
    )</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.train_len"><code class="name flex">
<span>def <span class="ident">train_len</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_len(self):
    return len(self.train_random_sampler)</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Validation data are in the variable train_data
(but the indices are splitted to self.val_sampler)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataLoader</code></dt>
<dd>The validation DataLoader</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    &#34;&#34;&#34;
    Validation data are in the variable train_data 
    (but the indices are splitted to self.val_sampler)

    Returns:
        DataLoader: The validation DataLoader
    &#34;&#34;&#34;   
    return DataLoader(
        self.train_dataset,
        sampler=self.val_sampler,
        batch_size=self.batch_size,
        num_workers=self.num_workers,
    )</code></pre>
</details>
</dd>
<dt id="Antolik_dataset.AntolikDataModule.val_len"><code class="name flex">
<span>def <span class="ident">val_len</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_len(self):
    return len(self.val_sampler)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Antolik_dataset.AntolikDataset"><code class="flex name class">
<span>class <span class="ident">AntolikDataset</span></span>
<span>(</span><span>path, normalize=True)</span>
</code></dt>
<dd>
<div class="desc"><p>A class for handling with the Antolik's synthetic dataset.</p>
<p>The constructor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the dataset</dd>
<dt><strong><code>normalize</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to normalize the images. Defaults to True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AntolikDataset(Dataset):
    &#34;&#34;&#34;A class for handling with the Antolik&#39;s synthetic dataset.&#34;&#34;&#34;    

    def __init__(self, path, normalize=True):
        &#34;&#34;&#34;The constructor.

        Args:
            path (str): Path to the dataset
            normalize (bool, optional): Whether to normalize the images. Defaults to True.
        &#34;&#34;&#34;
        self.normalize = normalize

        self.data = self.pickle_read(path)

        self.transform_list = transforms.Compose(
            [transforms.Normalize((45.2315,), (26.6845,))]
        )

    def __getitem__(self, index):
        &#34;&#34;&#34;Gets the index-th pair of visual stimulus and response to the stimulus.

        Args:
            index (int): the index

        Returns:
            (np.array, np.array): The index-th pair of visual stimulus and response to the stimulus.
        &#34;&#34;&#34;
        x = self.data[index][&#34;stimulus&#34;]
        x = np.expand_dims(x, axis=0)
        y = np.concatenate(
            [self.data[index][&#34;V1_Exc_L2/3&#34;], self.data[index][&#34;V1_Inh_L2/3&#34;]]
        )

        data = torch.from_numpy(x)
        target = torch.from_numpy(y)

        if self.normalize:
            data = self.transform_list(data)

        return (data.float(), target.float())

    def __len__(self):
        &#34;&#34;&#34;

        Returns:
            int: The length of the dataset
        &#34;&#34;&#34;
        return len(self.data)

    def pickle_read(self, path):
        &#34;&#34;&#34;A helper function to unpickle the dataset.

        Args:
            path (str): Path to the dataset

        Returns:
            The dataset.
        &#34;&#34;&#34;
        with open(path, &#34;rb&#34;) as f:
            x = pickle.load(f)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Antolik_dataset.AntolikDataset.pickle_read"><code class="name flex">
<span>def <span class="ident">pickle_read</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>A helper function to unpickle the dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pickle_read(self, path):
    &#34;&#34;&#34;A helper function to unpickle the dataset.

    Args:
        path (str): Path to the dataset

    Returns:
        The dataset.
    &#34;&#34;&#34;
    with open(path, &#34;rb&#34;) as f:
        x = pickle.load(f)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Antolik_dataset.AntolikDataModule" href="#Antolik_dataset.AntolikDataModule">AntolikDataModule</a></code></h4>
<ul class="">
<li><code><a title="Antolik_dataset.AntolikDataModule.get_input_shape" href="#Antolik_dataset.AntolikDataModule.get_input_shape">get_input_shape</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.get_mean" href="#Antolik_dataset.AntolikDataModule.get_mean">get_mean</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.get_oracle_dataloader" href="#Antolik_dataset.AntolikDataModule.get_oracle_dataloader">get_oracle_dataloader</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.get_output_shape" href="#Antolik_dataset.AntolikDataModule.get_output_shape">get_output_shape</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.model_performances" href="#Antolik_dataset.AntolikDataModule.model_performances">model_performances</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.name" href="#Antolik_dataset.AntolikDataModule.name">name</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.pickle_read" href="#Antolik_dataset.AntolikDataModule.pickle_read">pickle_read</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.prepare_data" href="#Antolik_dataset.AntolikDataModule.prepare_data">prepare_data</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.print_dataset_info" href="#Antolik_dataset.AntolikDataModule.print_dataset_info">print_dataset_info</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.setup" href="#Antolik_dataset.AntolikDataModule.setup">setup</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.test_dataloader" href="#Antolik_dataset.AntolikDataModule.test_dataloader">test_dataloader</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.test_len" href="#Antolik_dataset.AntolikDataModule.test_len">test_len</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.train_dataloader" href="#Antolik_dataset.AntolikDataModule.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.train_len" href="#Antolik_dataset.AntolikDataModule.train_len">train_len</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.val_dataloader" href="#Antolik_dataset.AntolikDataModule.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="Antolik_dataset.AntolikDataModule.val_len" href="#Antolik_dataset.AntolikDataModule.val_len">val_len</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Antolik_dataset.AntolikDataset" href="#Antolik_dataset.AntolikDataset">AntolikDataset</a></code></h4>
<ul class="">
<li><code><a title="Antolik_dataset.AntolikDataset.pickle_read" href="#Antolik_dataset.AntolikDataset.pickle_read">pickle_read</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>