<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>readout API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>readout</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from predict_neural_responses.models import *
import torch
from torch.nn import Parameter
from torch.nn import functional as F
import warnings


class Gaussian3dCyclic(readouts.Readout):
    &#34;&#34;&#34;
    This readout instantiates an object that can be used to learn a point in the core feature space for each neuron,
    sampled from a Gaussian distribution with some mean and variance at train but set to mean at test time, that best predicts its response.

    In the third dimension, the Gaussian distribution is cyclical, therefore when it gets out of the range [-1, 1], it gets
    back to a cyclically corresponding position (1.5 = -0.5).

    The readout receives the shape of the core as &#39;in_shape&#39;, the number of units/neurons being predicted as &#39;outdims&#39;, &#39;bias&#39; specifying whether
    or not bias term is to be used and &#39;init_range&#39; range for initialising the mean and variance of the gaussian distribution from which we sample to
    uniform distribution, U(-init_mu_range,init_mu_range) and  uniform distribution, U(0.0, init_sigma_range) respectively.
    The grid parameter contains the normalized locations (x, y coordinates in the core feature space) and is clipped to [-1.1] as it a
    requirement of the torch.grid_sample function. The third parameter of the grid is the orientation, which is cyclical, as
    stated above. The feature parameter learns the best linear mapping between the feature
    map from a given location, sample from Gaussian at train time but set to mean at eval time, and the unit&#39;s response with or without an additional elu non-linearity.

    Args:
        in_shape (list): shape of the input feature map [channels, width, height]
        outdims (int): number of output units
        bias (bool): adds a bias term
        init_mu_range (float): initialises the the mean with Uniform([-init_range, init_range])
                            [expected: positive value &lt;=1]
        init_sigma_range (float): initialises sigma with Uniform([0.0, init_sigma_range]).
                It is recommended however to use a fixed initialization, for faster convergence.
                For this, set fixed_sigma to True.
        batch_sample (bool): if True, samples a position for each image in the batch separately
                            [default: True as it decreases convergence time and performs just as well]
        fixed_sigma (bool). Recommended behavior: True. But set to false for backwards compatibility.
                If true, initialized the sigma not in a range, but with the exact value given for all neurons.
    &#34;&#34;&#34;

    def __init__(
        self,
        in_shape,
        outdims,
        bias,
        init_mu_range,
        init_sigma_range,
        batch_sample=True,
        fixed_sigma=False,
        mean_activity=None,
        feature_reg_weight=1.0,
        gamma_readout=None,  # depricated, use feature_reg_weight instead
        **kwargs,
    ):
        &#34;&#34;&#34;The constructor

        Args:
            in_shape (list): shape of the input feature map [channels, width, height]
            outdims (int): number of output units
            bias (bool): adds a bias term
            init_mu_range (float): initialises the the mean with Uniform([-init_range, init_range])
                                [expected: positive value &lt;=1]
            init_sigma_range (float): initialises sigma with Uniform([0.0, init_sigma_range]).
                    It is recommended however to use a fixed initialization, for faster convergence.
                    For this, set fixed_sigma to True.
            batch_sample (bool): if True, samples a position for each image in the batch separately
                                [default: True as it decreases convergence time and performs just as well]
            fixed_sigma (bool). Recommended behavior: True. But set to false for backwards compatibility.
                    If true, initialized the sigma not in a range, but with the exact value given for all neurons.
            mean_activity (tensor, optional): Tensor of mean activity of the neurons. Defaults to None.
            feature_reg_weight (float, optional): Regularization strength for the readout. Defaults to 1.0.
            gamma_readout (float, optional): Regularization for the readout. DO NOT USE, DEPRECATED Defaults to None.

        Raises:
            ValueError: If init_mu_range or init_sigma_range are not within required limit
        &#34;&#34;&#34;
        super().__init__()
        if init_mu_range &gt; 1.0 or init_mu_range &lt;= 0.0 or init_sigma_range &lt;= 0.0:
            raise ValueError(
                &#34;init_mu_range or init_sigma_range is not within required limit!&#34;
            )
        self.in_shape = in_shape
        self.outdims = outdims
        self.feature_reg_weight = self.resolve_deprecated_gamma_readout(
            feature_reg_weight, gamma_readout
        )
        self.batch_sample = batch_sample
        self.grid_shape = (1, 1, outdims, 1, 3)
        self.mu = Parameter(
            torch.Tensor(*self.grid_shape)
        )  # mean location of gaussian for each neuron
        self.sigma = Parameter(
            torch.Tensor(*self.grid_shape)
        )  # standard deviation for gaussian for each neuron
        self.features = Parameter(
            torch.Tensor(1, 1, 1, outdims)
        )  # saliency weights for each channel from core
        self.mean_activity = mean_activity
        if bias:
            bias = Parameter(torch.Tensor(outdims))
            self.register_parameter(&#34;bias&#34;, bias)
        else:
            self.register_parameter(&#34;bias&#34;, None)

        self.init_mu_range = init_mu_range
        self.init_sigma_range = init_sigma_range
        self.fixed_sigma = fixed_sigma
        self.initialize(mean_activity)

    def sample_grid(self, batch_size, sample=None): # significantly edited
        &#34;&#34;&#34;Returns the grid locations from the core by sampling from a Gaussian distribution
        
        Args:
            batch_size (int): size of the batch
            sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
                            or use the mean, mu, of the Gaussian distribution without sampling.
                           if sample is None (default), samples from the N(mu,sigma) during training phase and
                             fixes to the mean, mu, during evaluation phase.
                           if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed
        &#34;&#34;&#34;

        # We won&#39;t clamp it to the interval [-1, 1]
        with torch.no_grad():
            #     self.mu.clamp_(min=-1, max=1)  # at eval time, only self.mu is used so it must belong to [-1,1]
            self.sigma.clamp_(min=0)  # sigma/variance is always a positive quantity

        grid_shape = (batch_size,) + self.grid_shape[1:]

        sample = self.training if sample is None else sample

        if sample:
            norm = self.mu.new(*grid_shape).normal_()
        else:
            norm = self.mu.new(
                *grid_shape
            ).zero_()  # for consistency and CUDA capability

        corrected_distribution = norm * self.sigma + self.mu

        # move the interval [-1, 1] by 1 to right to [0, 2], but values can be somewhere else (not in these intervals)
        # then compute the reminder when dividing by 2 (= move the values into the [0, 2] interval)
        # then move the interval back to [-1, 1] by subtracting 1
        # .. now we will have every dimension periodic
        all_periodic = torch.remainder((corrected_distribution + 1), 2) - 1

        # we clamp everything into the [-1, 1] interval
        all_clamped = torch.clamp(corrected_distribution, min=-1, max=1)

        # but we want to clamp only x and y dimensions and not z (channel) dimension,
        # therefore in all_clamped we replace clamped channels by periodic channels
        all_clamped[:, :, :, :, 2] = all_periodic[:, :, :, :, 2]

        return all_clamped

    @property
    def grid(self):
        return self.sample_grid(batch_size=1, sample=False)

    def initialize(self, mean_activity=None):
        &#34;&#34;&#34;Initializes the readout.

        Args:
            mean_activity (tensor, optional): Tensor of mean activity of the neurons. Defaults to None.
        &#34;&#34;&#34;
        if mean_activity is None:
            mean_activity = self.mean_activity
        self.mu.data.uniform_(-self.init_mu_range, self.init_mu_range)
        if self.fixed_sigma:
            self.sigma.data.uniform_(self.init_sigma_range, self.init_sigma_range)
        else:
            self.sigma.data.uniform_(0, self.init_sigma_range)
            warnings.warn(
                &#34;sigma is sampled from uniform distribuiton, instead of a fixed value. Consider setting &#34;
                &#34;fixed_sigma to True&#34;
            )
        self.features.data.fill_(1 / self.in_shape[0])
        if self.bias is not None:
            self.initialize_bias(mean_activity=mean_activity)

    def regularizer(self, reduction=&#34;sum&#34;, average=None):
        return 0

    def forward(self, x, sample=None, shift=None, out_idx=None, **kwargs): # edited
        &#34;&#34;&#34;Propagates the input forwards through the readout
        
        Args:
            x: input data
            sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
                            or use the mean, mu, of the Gaussian distribution without sampling.
                           if sample is None (default), samples from the N(mu,sigma) during training phase and
                             fixes to the mean, mu, during evaluation phase.
                           if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed
            shift (bool): shifts the location of the grid (from eye-tracking data)
            out_idx (bool): index of neurons to be predicted

        Returns:
            y (tensor): neuronal activity
        &#34;&#34;&#34;

        N, c, w, h = x.size()
        c_in, w_in, h_in = self.in_shape
        if (c_in, w_in, h_in) != (c, w, h):
            raise ValueError(
                &#34;the specified feature map dimension is not the readout&#39;s expected input dimension&#34;
            )

        # we copy the first channel to the end to make it periodic
        with_copied_first_orientation = torch.cat(
            [x, x[:, 0, :, :].view(N, 1, w, h)], dim=1
        )

        with_copied_first_orientation = with_copied_first_orientation.view(
            N, 1, c + 1, w, h
        )

        feat = self.features
        bias = self.bias
        outdims = self.outdims

        if self.batch_sample:
            # sample the grid_locations separately per image per batch
            grid = self.sample_grid(
                batch_size=N, sample=sample
            )  # sample determines sampling from Gaussian
        else:
            # use one sampled grid_locations for all images in the batch
            grid = self.sample_grid(batch_size=1, sample=sample).expand(
                N, outdims, 1, 3
            )

        if out_idx is not None:
            # out_idx specifies the indices to subset of neurons for training/testing
            if isinstance(out_idx, np.ndarray):
                if out_idx.dtype == bool:
                    out_idx = np.where(out_idx)[0]
            feat = feat[:, :, :, out_idx]
            grid = grid[:, :, out_idx]
            if bias is not None:
                bias = bias[out_idx]
            outdims = len(out_idx)

        if shift is not None:
            grid = grid + shift[:, None, None, :]

        #  - Gets values from a grid
        #  - align_corners=True, because we need to have -1 as the center of the
        #    orientation 0 and value 1 mapped to the center of (again) orientation
        #    0, but it is copied at the end
        y = F.grid_sample(
            with_copied_first_orientation,
            grid,
            align_corners=True,
            padding_mode=&#34;border&#34;,
            mode=&#34;bilinear&#34;,
        )

        # reshapes to a better shape
        y = (y.squeeze(-1) * feat).sum(1).view(N, outdims)

        if self.bias is not None:
            y = y + bias
        return y</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="readout.Gaussian3dCyclic"><code class="flex name class">
<span>class <span class="ident">Gaussian3dCyclic</span></span>
<span>(</span><span>in_shape, outdims, bias, init_mu_range, init_sigma_range, batch_sample=True, fixed_sigma=False, mean_activity=None, feature_reg_weight=1.0, gamma_readout=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This readout instantiates an object that can be used to learn a point in the core feature space for each neuron,
sampled from a Gaussian distribution with some mean and variance at train but set to mean at test time, that best predicts its response.</p>
<p>In the third dimension, the Gaussian distribution is cyclical, therefore when it gets out of the range [-1, 1], it gets
back to a cyclically corresponding position (1.5 = -0.5).</p>
<p>The readout receives the shape of the core as 'in_shape', the number of units/neurons being predicted as 'outdims', 'bias' specifying whether
or not bias term is to be used and 'init_range' range for initialising the mean and variance of the gaussian distribution from which we sample to
uniform distribution, U(-init_mu_range,init_mu_range) and
uniform distribution, U(0.0, init_sigma_range) respectively.
The grid parameter contains the normalized locations (x, y coordinates in the core feature space) and is clipped to [-1.1] as it a
requirement of the torch.grid_sample function. The third parameter of the grid is the orientation, which is cyclical, as
stated above. The feature parameter learns the best linear mapping between the feature
map from a given location, sample from Gaussian at train time but set to mean at eval time, and the unit's response with or without an additional elu non-linearity.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>in_shape</code></strong> :&ensp;<code>list</code></dt>
<dd>shape of the input feature map [channels, width, height]</dd>
<dt><strong><code>outdims</code></strong> :&ensp;<code>int</code></dt>
<dd>number of output units</dd>
<dt><strong><code>bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>adds a bias term</dd>
<dt><strong><code>init_mu_range</code></strong> :&ensp;<code>float</code></dt>
<dd>initialises the the mean with Uniform([-init_range, init_range])
[expected: positive value &lt;=1]</dd>
<dt><strong><code>init_sigma_range</code></strong> :&ensp;<code>float</code></dt>
<dd>initialises sigma with Uniform([0.0, init_sigma_range]).
It is recommended however to use a fixed initialization, for faster convergence.
For this, set fixed_sigma to True.</dd>
<dt><strong><code>batch_sample</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, samples a position for each image in the batch separately
[default: True as it decreases convergence time and performs just as well]</dd>
</dl>
<p>fixed_sigma (bool). Recommended behavior: True. But set to false for backwards compatibility.
If true, initialized the sigma not in a range, but with the exact value given for all neurons.
The constructor</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>in_shape</code></strong> :&ensp;<code>list</code></dt>
<dd>shape of the input feature map [channels, width, height]</dd>
<dt><strong><code>outdims</code></strong> :&ensp;<code>int</code></dt>
<dd>number of output units</dd>
<dt><strong><code>bias</code></strong> :&ensp;<code>bool</code></dt>
<dd>adds a bias term</dd>
<dt><strong><code>init_mu_range</code></strong> :&ensp;<code>float</code></dt>
<dd>initialises the the mean with Uniform([-init_range, init_range])
[expected: positive value &lt;=1]</dd>
<dt><strong><code>init_sigma_range</code></strong> :&ensp;<code>float</code></dt>
<dd>initialises sigma with Uniform([0.0, init_sigma_range]).
It is recommended however to use a fixed initialization, for faster convergence.
For this, set fixed_sigma to True.</dd>
<dt><strong><code>batch_sample</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, samples a position for each image in the batch separately
[default: True as it decreases convergence time and performs just as well]</dd>
<dt>fixed_sigma (bool). Recommended behavior: True. But set to false for backwards compatibility.</dt>
<dt>If true, initialized the sigma not in a range, but with the exact value given for all neurons.</dt>
<dt><strong><code>mean_activity</code></strong> :&ensp;<code>tensor</code>, optional</dt>
<dd>Tensor of mean activity of the neurons. Defaults to None.</dd>
<dt><strong><code>feature_reg_weight</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Regularization strength for the readout. Defaults to 1.0.</dd>
<dt><strong><code>gamma_readout</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Regularization for the readout. DO NOT USE, DEPRECATED Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If init_mu_range or init_sigma_range are not within required limit</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gaussian3dCyclic(readouts.Readout):
    &#34;&#34;&#34;
    This readout instantiates an object that can be used to learn a point in the core feature space for each neuron,
    sampled from a Gaussian distribution with some mean and variance at train but set to mean at test time, that best predicts its response.

    In the third dimension, the Gaussian distribution is cyclical, therefore when it gets out of the range [-1, 1], it gets
    back to a cyclically corresponding position (1.5 = -0.5).

    The readout receives the shape of the core as &#39;in_shape&#39;, the number of units/neurons being predicted as &#39;outdims&#39;, &#39;bias&#39; specifying whether
    or not bias term is to be used and &#39;init_range&#39; range for initialising the mean and variance of the gaussian distribution from which we sample to
    uniform distribution, U(-init_mu_range,init_mu_range) and  uniform distribution, U(0.0, init_sigma_range) respectively.
    The grid parameter contains the normalized locations (x, y coordinates in the core feature space) and is clipped to [-1.1] as it a
    requirement of the torch.grid_sample function. The third parameter of the grid is the orientation, which is cyclical, as
    stated above. The feature parameter learns the best linear mapping between the feature
    map from a given location, sample from Gaussian at train time but set to mean at eval time, and the unit&#39;s response with or without an additional elu non-linearity.

    Args:
        in_shape (list): shape of the input feature map [channels, width, height]
        outdims (int): number of output units
        bias (bool): adds a bias term
        init_mu_range (float): initialises the the mean with Uniform([-init_range, init_range])
                            [expected: positive value &lt;=1]
        init_sigma_range (float): initialises sigma with Uniform([0.0, init_sigma_range]).
                It is recommended however to use a fixed initialization, for faster convergence.
                For this, set fixed_sigma to True.
        batch_sample (bool): if True, samples a position for each image in the batch separately
                            [default: True as it decreases convergence time and performs just as well]
        fixed_sigma (bool). Recommended behavior: True. But set to false for backwards compatibility.
                If true, initialized the sigma not in a range, but with the exact value given for all neurons.
    &#34;&#34;&#34;

    def __init__(
        self,
        in_shape,
        outdims,
        bias,
        init_mu_range,
        init_sigma_range,
        batch_sample=True,
        fixed_sigma=False,
        mean_activity=None,
        feature_reg_weight=1.0,
        gamma_readout=None,  # depricated, use feature_reg_weight instead
        **kwargs,
    ):
        &#34;&#34;&#34;The constructor

        Args:
            in_shape (list): shape of the input feature map [channels, width, height]
            outdims (int): number of output units
            bias (bool): adds a bias term
            init_mu_range (float): initialises the the mean with Uniform([-init_range, init_range])
                                [expected: positive value &lt;=1]
            init_sigma_range (float): initialises sigma with Uniform([0.0, init_sigma_range]).
                    It is recommended however to use a fixed initialization, for faster convergence.
                    For this, set fixed_sigma to True.
            batch_sample (bool): if True, samples a position for each image in the batch separately
                                [default: True as it decreases convergence time and performs just as well]
            fixed_sigma (bool). Recommended behavior: True. But set to false for backwards compatibility.
                    If true, initialized the sigma not in a range, but with the exact value given for all neurons.
            mean_activity (tensor, optional): Tensor of mean activity of the neurons. Defaults to None.
            feature_reg_weight (float, optional): Regularization strength for the readout. Defaults to 1.0.
            gamma_readout (float, optional): Regularization for the readout. DO NOT USE, DEPRECATED Defaults to None.

        Raises:
            ValueError: If init_mu_range or init_sigma_range are not within required limit
        &#34;&#34;&#34;
        super().__init__()
        if init_mu_range &gt; 1.0 or init_mu_range &lt;= 0.0 or init_sigma_range &lt;= 0.0:
            raise ValueError(
                &#34;init_mu_range or init_sigma_range is not within required limit!&#34;
            )
        self.in_shape = in_shape
        self.outdims = outdims
        self.feature_reg_weight = self.resolve_deprecated_gamma_readout(
            feature_reg_weight, gamma_readout
        )
        self.batch_sample = batch_sample
        self.grid_shape = (1, 1, outdims, 1, 3)
        self.mu = Parameter(
            torch.Tensor(*self.grid_shape)
        )  # mean location of gaussian for each neuron
        self.sigma = Parameter(
            torch.Tensor(*self.grid_shape)
        )  # standard deviation for gaussian for each neuron
        self.features = Parameter(
            torch.Tensor(1, 1, 1, outdims)
        )  # saliency weights for each channel from core
        self.mean_activity = mean_activity
        if bias:
            bias = Parameter(torch.Tensor(outdims))
            self.register_parameter(&#34;bias&#34;, bias)
        else:
            self.register_parameter(&#34;bias&#34;, None)

        self.init_mu_range = init_mu_range
        self.init_sigma_range = init_sigma_range
        self.fixed_sigma = fixed_sigma
        self.initialize(mean_activity)

    def sample_grid(self, batch_size, sample=None): # significantly edited
        &#34;&#34;&#34;Returns the grid locations from the core by sampling from a Gaussian distribution
        
        Args:
            batch_size (int): size of the batch
            sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
                            or use the mean, mu, of the Gaussian distribution without sampling.
                           if sample is None (default), samples from the N(mu,sigma) during training phase and
                             fixes to the mean, mu, during evaluation phase.
                           if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed
        &#34;&#34;&#34;

        # We won&#39;t clamp it to the interval [-1, 1]
        with torch.no_grad():
            #     self.mu.clamp_(min=-1, max=1)  # at eval time, only self.mu is used so it must belong to [-1,1]
            self.sigma.clamp_(min=0)  # sigma/variance is always a positive quantity

        grid_shape = (batch_size,) + self.grid_shape[1:]

        sample = self.training if sample is None else sample

        if sample:
            norm = self.mu.new(*grid_shape).normal_()
        else:
            norm = self.mu.new(
                *grid_shape
            ).zero_()  # for consistency and CUDA capability

        corrected_distribution = norm * self.sigma + self.mu

        # move the interval [-1, 1] by 1 to right to [0, 2], but values can be somewhere else (not in these intervals)
        # then compute the reminder when dividing by 2 (= move the values into the [0, 2] interval)
        # then move the interval back to [-1, 1] by subtracting 1
        # .. now we will have every dimension periodic
        all_periodic = torch.remainder((corrected_distribution + 1), 2) - 1

        # we clamp everything into the [-1, 1] interval
        all_clamped = torch.clamp(corrected_distribution, min=-1, max=1)

        # but we want to clamp only x and y dimensions and not z (channel) dimension,
        # therefore in all_clamped we replace clamped channels by periodic channels
        all_clamped[:, :, :, :, 2] = all_periodic[:, :, :, :, 2]

        return all_clamped

    @property
    def grid(self):
        return self.sample_grid(batch_size=1, sample=False)

    def initialize(self, mean_activity=None):
        &#34;&#34;&#34;Initializes the readout.

        Args:
            mean_activity (tensor, optional): Tensor of mean activity of the neurons. Defaults to None.
        &#34;&#34;&#34;
        if mean_activity is None:
            mean_activity = self.mean_activity
        self.mu.data.uniform_(-self.init_mu_range, self.init_mu_range)
        if self.fixed_sigma:
            self.sigma.data.uniform_(self.init_sigma_range, self.init_sigma_range)
        else:
            self.sigma.data.uniform_(0, self.init_sigma_range)
            warnings.warn(
                &#34;sigma is sampled from uniform distribuiton, instead of a fixed value. Consider setting &#34;
                &#34;fixed_sigma to True&#34;
            )
        self.features.data.fill_(1 / self.in_shape[0])
        if self.bias is not None:
            self.initialize_bias(mean_activity=mean_activity)

    def regularizer(self, reduction=&#34;sum&#34;, average=None):
        return 0

    def forward(self, x, sample=None, shift=None, out_idx=None, **kwargs): # edited
        &#34;&#34;&#34;Propagates the input forwards through the readout
        
        Args:
            x: input data
            sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
                            or use the mean, mu, of the Gaussian distribution without sampling.
                           if sample is None (default), samples from the N(mu,sigma) during training phase and
                             fixes to the mean, mu, during evaluation phase.
                           if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed
            shift (bool): shifts the location of the grid (from eye-tracking data)
            out_idx (bool): index of neurons to be predicted

        Returns:
            y (tensor): neuronal activity
        &#34;&#34;&#34;

        N, c, w, h = x.size()
        c_in, w_in, h_in = self.in_shape
        if (c_in, w_in, h_in) != (c, w, h):
            raise ValueError(
                &#34;the specified feature map dimension is not the readout&#39;s expected input dimension&#34;
            )

        # we copy the first channel to the end to make it periodic
        with_copied_first_orientation = torch.cat(
            [x, x[:, 0, :, :].view(N, 1, w, h)], dim=1
        )

        with_copied_first_orientation = with_copied_first_orientation.view(
            N, 1, c + 1, w, h
        )

        feat = self.features
        bias = self.bias
        outdims = self.outdims

        if self.batch_sample:
            # sample the grid_locations separately per image per batch
            grid = self.sample_grid(
                batch_size=N, sample=sample
            )  # sample determines sampling from Gaussian
        else:
            # use one sampled grid_locations for all images in the batch
            grid = self.sample_grid(batch_size=1, sample=sample).expand(
                N, outdims, 1, 3
            )

        if out_idx is not None:
            # out_idx specifies the indices to subset of neurons for training/testing
            if isinstance(out_idx, np.ndarray):
                if out_idx.dtype == bool:
                    out_idx = np.where(out_idx)[0]
            feat = feat[:, :, :, out_idx]
            grid = grid[:, :, out_idx]
            if bias is not None:
                bias = bias[out_idx]
            outdims = len(out_idx)

        if shift is not None:
            grid = grid + shift[:, None, None, :]

        #  - Gets values from a grid
        #  - align_corners=True, because we need to have -1 as the center of the
        #    orientation 0 and value 1 mapped to the center of (again) orientation
        #    0, but it is copied at the end
        y = F.grid_sample(
            with_copied_first_orientation,
            grid,
            align_corners=True,
            padding_mode=&#34;border&#34;,
            mode=&#34;bilinear&#34;,
        )

        # reshapes to a better shape
        y = (y.squeeze(-1) * feat).sum(1).view(N, outdims)

        if self.bias is not None:
            y = y + bias
        return y</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>neuralpredictors.layers.readouts.base.Readout</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="readout.Gaussian3dCyclic.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="readout.Gaussian3dCyclic.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="readout.Gaussian3dCyclic.grid"><code class="name">var <span class="ident">grid</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def grid(self):
    return self.sample_grid(batch_size=1, sample=False)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="readout.Gaussian3dCyclic.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, sample=None, shift=None, out_idx=None, **kwargs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Propagates the input forwards through the readout</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>input data</dd>
<dt>sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron</dt>
<dt>or use the mean, mu, of the Gaussian distribution without sampling.</dt>
<dt>if sample is None (default), samples from the N(mu,sigma) during training phase and</dt>
<dt>fixes to the mean, mu, during evaluation phase.</dt>
<dt>if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed</dt>
<dt><strong><code>shift</code></strong> :&ensp;<code>bool</code></dt>
<dd>shifts the location of the grid (from eye-tracking data)</dd>
<dt><strong><code>out_idx</code></strong> :&ensp;<code>bool</code></dt>
<dd>index of neurons to be predicted</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>y (tensor): neuronal activity</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, sample=None, shift=None, out_idx=None, **kwargs): # edited
    &#34;&#34;&#34;Propagates the input forwards through the readout
    
    Args:
        x: input data
        sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
                        or use the mean, mu, of the Gaussian distribution without sampling.
                       if sample is None (default), samples from the N(mu,sigma) during training phase and
                         fixes to the mean, mu, during evaluation phase.
                       if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed
        shift (bool): shifts the location of the grid (from eye-tracking data)
        out_idx (bool): index of neurons to be predicted

    Returns:
        y (tensor): neuronal activity
    &#34;&#34;&#34;

    N, c, w, h = x.size()
    c_in, w_in, h_in = self.in_shape
    if (c_in, w_in, h_in) != (c, w, h):
        raise ValueError(
            &#34;the specified feature map dimension is not the readout&#39;s expected input dimension&#34;
        )

    # we copy the first channel to the end to make it periodic
    with_copied_first_orientation = torch.cat(
        [x, x[:, 0, :, :].view(N, 1, w, h)], dim=1
    )

    with_copied_first_orientation = with_copied_first_orientation.view(
        N, 1, c + 1, w, h
    )

    feat = self.features
    bias = self.bias
    outdims = self.outdims

    if self.batch_sample:
        # sample the grid_locations separately per image per batch
        grid = self.sample_grid(
            batch_size=N, sample=sample
        )  # sample determines sampling from Gaussian
    else:
        # use one sampled grid_locations for all images in the batch
        grid = self.sample_grid(batch_size=1, sample=sample).expand(
            N, outdims, 1, 3
        )

    if out_idx is not None:
        # out_idx specifies the indices to subset of neurons for training/testing
        if isinstance(out_idx, np.ndarray):
            if out_idx.dtype == bool:
                out_idx = np.where(out_idx)[0]
        feat = feat[:, :, :, out_idx]
        grid = grid[:, :, out_idx]
        if bias is not None:
            bias = bias[out_idx]
        outdims = len(out_idx)

    if shift is not None:
        grid = grid + shift[:, None, None, :]

    #  - Gets values from a grid
    #  - align_corners=True, because we need to have -1 as the center of the
    #    orientation 0 and value 1 mapped to the center of (again) orientation
    #    0, but it is copied at the end
    y = F.grid_sample(
        with_copied_first_orientation,
        grid,
        align_corners=True,
        padding_mode=&#34;border&#34;,
        mode=&#34;bilinear&#34;,
    )

    # reshapes to a better shape
    y = (y.squeeze(-1) * feat).sum(1).view(N, outdims)

    if self.bias is not None:
        y = y + bias
    return y</code></pre>
</details>
</dd>
<dt id="readout.Gaussian3dCyclic.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, mean_activity=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Initializes the readout.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mean_activity</code></strong> :&ensp;<code>tensor</code>, optional</dt>
<dd>Tensor of mean activity of the neurons. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, mean_activity=None):
    &#34;&#34;&#34;Initializes the readout.

    Args:
        mean_activity (tensor, optional): Tensor of mean activity of the neurons. Defaults to None.
    &#34;&#34;&#34;
    if mean_activity is None:
        mean_activity = self.mean_activity
    self.mu.data.uniform_(-self.init_mu_range, self.init_mu_range)
    if self.fixed_sigma:
        self.sigma.data.uniform_(self.init_sigma_range, self.init_sigma_range)
    else:
        self.sigma.data.uniform_(0, self.init_sigma_range)
        warnings.warn(
            &#34;sigma is sampled from uniform distribuiton, instead of a fixed value. Consider setting &#34;
            &#34;fixed_sigma to True&#34;
        )
    self.features.data.fill_(1 / self.in_shape[0])
    if self.bias is not None:
        self.initialize_bias(mean_activity=mean_activity)</code></pre>
</details>
</dd>
<dt id="readout.Gaussian3dCyclic.regularizer"><code class="name flex">
<span>def <span class="ident">regularizer</span></span>(<span>self, reduction='sum', average=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regularizer(self, reduction=&#34;sum&#34;, average=None):
    return 0</code></pre>
</details>
</dd>
<dt id="readout.Gaussian3dCyclic.sample_grid"><code class="name flex">
<span>def <span class="ident">sample_grid</span></span>(<span>self, batch_size, sample=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the grid locations from the core by sampling from a Gaussian distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>size of the batch</dd>
</dl>
<p>sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
or use the mean, mu, of the Gaussian distribution without sampling.
if sample is None (default), samples from the N(mu,sigma) during training phase and
fixes to the mean, mu, during evaluation phase.
if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_grid(self, batch_size, sample=None): # significantly edited
    &#34;&#34;&#34;Returns the grid locations from the core by sampling from a Gaussian distribution
    
    Args:
        batch_size (int): size of the batch
        sample (bool/None): sample determines whether we draw a sample from Gaussian distribution, N(mu,sigma), defined per neuron
                        or use the mean, mu, of the Gaussian distribution without sampling.
                       if sample is None (default), samples from the N(mu,sigma) during training phase and
                         fixes to the mean, mu, during evaluation phase.
                       if sample is True/False, overrides the model_state (i.e training or eval) and does as instructed
    &#34;&#34;&#34;

    # We won&#39;t clamp it to the interval [-1, 1]
    with torch.no_grad():
        #     self.mu.clamp_(min=-1, max=1)  # at eval time, only self.mu is used so it must belong to [-1,1]
        self.sigma.clamp_(min=0)  # sigma/variance is always a positive quantity

    grid_shape = (batch_size,) + self.grid_shape[1:]

    sample = self.training if sample is None else sample

    if sample:
        norm = self.mu.new(*grid_shape).normal_()
    else:
        norm = self.mu.new(
            *grid_shape
        ).zero_()  # for consistency and CUDA capability

    corrected_distribution = norm * self.sigma + self.mu

    # move the interval [-1, 1] by 1 to right to [0, 2], but values can be somewhere else (not in these intervals)
    # then compute the reminder when dividing by 2 (= move the values into the [0, 2] interval)
    # then move the interval back to [-1, 1] by subtracting 1
    # .. now we will have every dimension periodic
    all_periodic = torch.remainder((corrected_distribution + 1), 2) - 1

    # we clamp everything into the [-1, 1] interval
    all_clamped = torch.clamp(corrected_distribution, min=-1, max=1)

    # but we want to clamp only x and y dimensions and not z (channel) dimension,
    # therefore in all_clamped we replace clamped channels by periodic channels
    all_clamped[:, :, :, :, 2] = all_periodic[:, :, :, :, 2]

    return all_clamped</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="readout.Gaussian3dCyclic" href="#readout.Gaussian3dCyclic">Gaussian3dCyclic</a></code></h4>
<ul class="two-column">
<li><code><a title="readout.Gaussian3dCyclic.dump_patches" href="#readout.Gaussian3dCyclic.dump_patches">dump_patches</a></code></li>
<li><code><a title="readout.Gaussian3dCyclic.forward" href="#readout.Gaussian3dCyclic.forward">forward</a></code></li>
<li><code><a title="readout.Gaussian3dCyclic.grid" href="#readout.Gaussian3dCyclic.grid">grid</a></code></li>
<li><code><a title="readout.Gaussian3dCyclic.initialize" href="#readout.Gaussian3dCyclic.initialize">initialize</a></code></li>
<li><code><a title="readout.Gaussian3dCyclic.regularizer" href="#readout.Gaussian3dCyclic.regularizer">regularizer</a></code></li>
<li><code><a title="readout.Gaussian3dCyclic.sample_grid" href="#readout.Gaussian3dCyclic.sample_grid">sample_grid</a></code></li>
<li><code><a title="readout.Gaussian3dCyclic.training" href="#readout.Gaussian3dCyclic.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>