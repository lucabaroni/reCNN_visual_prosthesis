{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from experiments.utils import pickle_read\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import ProgressBar\n",
    "import pytorch_lightning as pl\n",
    "from neuralpredictors.measures.modules import Corr, PoissonLoss\n",
    "from torch.nn import Parameter\n",
    "from energy_model.lucas_gabor_filter import GaborFilter\n",
    "from energy_model.utils import plot_f, create_grating\n",
    "from energy_model.energy_model import EnergyModel\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/auto/budejovice1/mpicek/reCNN_visual_prosthesis', '/auto/budejovice1/mpicek/reCNN_visual_prosthesis', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/auto/vestec1-elixir/home/mpicek/.ipython', '/auto/budejovice1/mpicek/reCNN_visual_prosthesis/predict_neural_responses']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5907.7554)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/neuralpredictors/measures/modules.py:78: UserWarning: Poissonloss is averaged per batch. It's recommended to use `sum` instead\n",
      "  warnings.warn(\"Poissonloss is averaged per batch. It's recommended to use `sum` instead\")\n"
     ]
    }
   ],
   "source": [
    "poiss = PoissonLoss()\n",
    "out = poiss(torch.ones(1)*1000, torch.ones(1)*1000)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENTITY = \"csng-cuni\"\n",
    "PROJECT = \"reCNN_visual_prosthesis\"\n",
    "ground_truth_positions_file_path = \"data/antolik/position_dictionary.pickle\"\n",
    "ground_truth_orientations_file_path = \"data/antolik/oris.pickle\"\n",
    "\n",
    "model = None\n",
    "\n",
    "config = {\n",
    "    # GENERAL\n",
    "    \"seed\": 2,\n",
    "    \"batch_size\": 10,\n",
    "    \"lr\": 0.01,\n",
    "    \"max_epochs\": 100,\n",
    "\n",
    "    # CORE GENERAL CONFIG\n",
    "    \"core_hidden_channels\": 8,\n",
    "    \"core_layers\": 5,\n",
    "    \"core_input_kern\": 7,\n",
    "    \"core_hidden_kern\": 9,\n",
    "\n",
    "    # ROTATION EQUIVARIANCE CORE CONFIG\n",
    "    \"num_rotations\": 8,       \n",
    "    \"stride\": 1,               \n",
    "    \"upsampling\": 2,           \n",
    "    \"rot_eq_batch_norm\": True, \n",
    "    \"stack\": -1 ,               \n",
    "    \"depth_separable\": True,\n",
    "\n",
    "    # READOUT CONFIG\n",
    "    \"readout_bias\": False,\n",
    "    \"nonlinearity\": \"softplus\",\n",
    "    \n",
    "    # REGULARIZATION\n",
    "    \"core_gamma_input\": 0.00307424496692959,\n",
    "    \"core_gamma_hidden\": 0.28463619129195233,\n",
    "    \"readout_gamma\": 0.17,\n",
    "    \"input_regularizer\": \"LaplaceL2norm\", # for RotEqCore - default \n",
    "    \"use_avg_reg\": True,\n",
    "\n",
    "    \"reg_readout_spatial_smoothness\": 0.0027,\n",
    "    \"reg_group_sparsity\": 0.1,\n",
    "    \"reg_spatial_sparsity\": 0.45,\n",
    "\n",
    "    # TRAINER\n",
    "    \"patience\": 7,\n",
    "    \"train_on_val\": False, # in case you want to quickly check that your model \"compiles\" correctly\n",
    "    \"test\": True,\n",
    "    \"observed_val_metric\": \"val/corr\",\n",
    "\n",
    "    \"test_average_batch\": False,\n",
    "    \"compute_oracle_fraction\": False,\n",
    "    \"conservative_oracle\": True,\n",
    "    \"jackknife_oracle\": True,\n",
    "    \"generate_oracle_figure\": False,\n",
    "\n",
    "    # ANTOLIK\n",
    "    \"region\": \"region1\",\n",
    "    \"dataset_artifact_name\": \"Antolik_dataset:latest\",\n",
    "\n",
    "    # BOTTLENECK\n",
    "    \"bottleneck_kernel\": 15,\n",
    "\n",
    "    \"fixed_sigma\": False,\n",
    "    \"init_mu_range\": 0.9,\n",
    "    \"init_sigma_range\": 0.8,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Antolik_dataset import AntolikDataModule\n",
    "\n",
    "path_train = \"/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized/one_trials.pickle\"\n",
    "path_test = \"/storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized/ten_trials.pickle\"\n",
    "\n",
    "dataset_config = {\n",
    "    \"train_data_dir\": path_train,\n",
    "    \"test_data_dir\": path_test,\n",
    "    \"batch_size\": config[\"batch_size\"],\n",
    "    \"normalize\": True,\n",
    "    \"val_size\": 500,\n",
    "    \"brain_crop\": None,\n",
    "    \"stimulus_crop\": None,\n",
    "    # \"brain_crop\": 0.8,\n",
    "    # \"stimulus_crop\": \"auto\",\n",
    "    # \"stimulus_crop\": [110, 110],\n",
    "    # \"ground_truth_positions_file_path\": \"data/antolik/position_dictionary.pickle\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "from neuralpredictors.data.samplers import SubsetSequentialSampler\n",
    "from typing import Optional\n",
    "import pathlib\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from experiments.utils import pickle_read\n",
    "from Antolik_dataset import AntolikDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = AntolikDataModule(**dataset_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded precomputed mean from /storage/brno2/home/mpicek/reCNN_visual_prosthesis/data/antolik_reparametrized/one_trials_mean.npy\n"
     ]
    }
   ],
   "source": [
    "config.update(\n",
    "        {\n",
    "            \"input_channels\": dm.get_input_shape()[0],\n",
    "            \"input_size_x\": dm.get_input_shape()[1],\n",
    "            \"input_size_y\": dm.get_input_shape()[2],\n",
    "            \"num_neurons\": dm.get_output_shape()[0],\n",
    "            \"mean_activity\": dm.get_mean(),\n",
    "            \"filtered_neurons\":dm.get_filtered_neurons(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# resolution = (dm.get_input_shape()[1], dm.get_input_shape()[2])\n",
    "# xlim = [-dm.get_stimulus_visual_angle()/2, dm.get_stimulus_visual_angle()/2]\n",
    "# ylim = [-dm.get_stimulus_visual_angle()/2, dm.get_stimulus_visual_angle()/2]\n",
    "\n",
    "# pos_x, pos_y, orientations = dm.get_ground_truth(ground_truth_positions_file_path, ground_truth_orientations_file_path)\n",
    "\n",
    "# model = EnergyModel(pos_x, pos_y, orientations, resolution, xlim, ylim, default_ori_shift=90, learning_rate=0.01, counter_clockwise_rotation=True, multivariate=True, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\n",
    "    {\n",
    "        # \"ground_truth_positions_file_path\": \"data/antolik/position_dictionary.pickle\",\n",
    "        # \"ground_truth_orientations_file_path\": \"data/antolik/oris.pickle\",\n",
    "        \"ground_truth_positions_file_path\": \"data/antolik/positions_reparametrized.pickle\",\n",
    "        \"ground_truth_orientations_file_path\": \"data/antolik/oris_reparametrized.pickle\",\n",
    "        \"init_to_ground_truth_positions\": False,\n",
    "        \"init_to_ground_truth_orientations\": False,\n",
    "        \"freeze_positions\": False,\n",
    "        \"freeze_orientations\": False,\n",
    "        \"orientation_shift\": 87.4,\n",
    "        \"factor\": 5.5,\n",
    "        \"sample\": False,\n",
    "        \"filtered_neurons\":None,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "early_stopping_monitor=\"val/corr\"\n",
    "early_stopping_mode=\"max\"\n",
    "model_checkpoint_monitor=\"val/corr\"\n",
    "model_checkpoint_mode=\"max\"\n",
    "\n",
    "use_wandb = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Antolik_dataset.AntolikDataModule"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/neuralpredictors/measures/modules.py:78: UserWarning: Poissonloss is averaged per batch. It's recommended to use `sum` instead\n",
      "  warnings.warn(\"Poissonloss is averaged per batch. It's recommended to use `sum` instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mame dataloader????\n",
      "tak cobude :(((\n",
      "nemame dataloader!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/neuralpredictors/layers/cores/conv2d.py:128: UserWarning: The averaged value of regularizer will be used.\n",
      "  warnings.warn(\"The averaged value of regularizer will be used.\", UserWarning)\n",
      "/auto/budejovice1/mpicek/reCNN_visual_prosthesis/readout.py:380: UserWarning: sigma is sampled from uniform distribuiton, instead of a fixed value. Consider setting fixed_sigma to True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models import reCNN_bottleneck_CyclicGauss3d_no_scaling\n",
    "\n",
    "config[\"positions_minus_x\"] = False\n",
    "config[\"positions_minus_y\"] = True\n",
    "config[\"do_not_sample\"] = True\n",
    "\n",
    "model_artifact_name = None\n",
    "needs_ground_truth = False\n",
    "model_needs_dataloader = True\n",
    "model_class = reCNN_bottleneck_CyclicGauss3d_no_scaling\n",
    "if needs_ground_truth:\n",
    "    pos_x, pos_y, orientations = dm.get_ground_truth(config[\"ground_truth_positions_file_path\"], config[\"ground_truth_orientations_file_path\"])\n",
    "    resolution = (dm.get_input_shape()[1], dm.get_input_shape()[2])\n",
    "    xlim = [-dm.get_stimulus_visual_angle()/2, dm.get_stimulus_visual_angle()/2]\n",
    "    ylim = [-dm.get_stimulus_visual_angle()/2, dm.get_stimulus_visual_angle()/2]\n",
    "    # model = model_class(pos_x, pos_y, orientations, resolution, xlim, ylim, **config)\n",
    "elif model_needs_dataloader:\n",
    "    model = model_class(dm, **config)\n",
    "else:\n",
    "    model = model_class(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"core_gamma_hidden\"] = 0.008931320307500908\n",
    "config[\"bottleneck_kernel\"] = 15\n",
    "config[\"core_gamma_input\"] = 0.2384005754453638\n",
    "config[\"core_hidden_channels\"] = 6\n",
    "config[\"core_hidden_kern\"] = 19\n",
    "config[\"core_input_kern\"] = 5\n",
    "config[\"core_layers\"] = 5\n",
    "config[\"depth_separable\"] = True\n",
    "config[\"lr\"] = 0.0005\n",
    "config[\"num_rotations\"] = 8\n",
    "config[\"upsampling\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(\n",
    "#     callbacks=[],\n",
    "#     max_epochs=config[\"max_epochs\"],\n",
    "#     gpus=[0],\n",
    "#     logger=False,\n",
    "#     log_every_n_steps=100,\n",
    "#     # deterministic=True,\n",
    "#     enable_checkpointing=True,\n",
    "#     # fast_dev_run=True,\n",
    "#     # fast_dev_run=7\n",
    "#     # limit_train_batches=1\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#     model,\n",
    "#     train_dataloaders=dm.train_dataloader(),\n",
    "#     val_dataloaders=dm.val_dataloader(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2lhzci1m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 944905... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-silence-23</strong>: <a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/2lhzci1m\" target=\"_blank\">https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/2lhzci1m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20230301_124248-2lhzci1m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2lhzci1m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/2ezcqnzf\" target=\"_blank\">lemon-morning-24</a></strong> to <a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-78fdadac-cb84-e2a1-5ae2-1111d6ed43f1]\n",
      "\n",
      "  | Name    | Type                                | Params\n",
      "----------------------------------------------------------------\n",
      "0 | loss    | PoissonLoss                         | 0     \n",
      "1 | corr    | Corr                                | 0     \n",
      "2 | core    | RotationEquivariant2dCoreBottleneck | 1.8 M \n",
      "3 | readout | Gaussian3dCyclicNoScale             | 35.0 K\n",
      "4 | nonlin  | Softplus                            | 0     \n",
      "----------------------------------------------------------------\n",
      "259 K     Trainable params\n",
      "1.6 M     Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.335     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:452: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  16%|█▌        | 729/4500 [12:57<1:07:04,  1.07s/it, loss=-0.537, v_num=qnzf]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import wandb\n",
    "from Lurz_dataset import LurzDataModule\n",
    "\n",
    "from models import reCNN_FullFactorized\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import ProgressBar\n",
    "import pytorch_lightning as pl\n",
    "from models import reCNN_bottleneck_CyclicGauss3d\n",
    "from pprint import pprint\n",
    "from Antolik_dataset import AntolikDataModule\n",
    "\n",
    "\n",
    "pl.seed_everything(config[\"seed\"], workers=True)    \n",
    "\n",
    "# init wandb run\n",
    "run = wandb.init(\n",
    "    config=config,\n",
    "    project=PROJECT,\n",
    "    entity=ENTITY,\n",
    ")\n",
    "\n",
    "# Access all hyperparameter values through wandb.config\n",
    "# config = dict(wandb.config)\n",
    "# pprint(config)\n",
    "\n",
    "\n",
    "# setup wandb logger\n",
    "wandb_logger = WandbLogger(log_model=True)\n",
    "wandb_logger.watch(model, log=\"parameters\", log_freq=250)\n",
    "\n",
    "# define callbacks for the training\n",
    "# early_stop = EarlyStopping(\n",
    "#     monitor=early_stopping_monitor,\n",
    "#     patience=config[\"patience\"],\n",
    "#     mode=early_stopping_mode,\n",
    "# )\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1, monitor=model_checkpoint_monitor, mode=model_checkpoint_mode\n",
    ")\n",
    "\n",
    "# class LitProgressBar(ProgressBar):\n",
    "#     def get_metrics(self, trainer, model):\n",
    "#         # don't show the version number\n",
    "#         items = super().get_metrics(trainer, model)\n",
    "#         items.pop(\"v_num\", None)\n",
    "#         return items\n",
    "\n",
    "# bar = LitProgressBar()\n",
    "\n",
    "\n",
    "# define the trainer\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # max_epochs=config[\"max_epochs\"],\n",
    "    max_time=timedelta(hours=4),\n",
    "    # max_epochs=1,\n",
    "    gpus=[0],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=250,\n",
    "    # deterministic=True,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=dm.train_dataloader(),\n",
    "    val_dataloaders=dm.val_dataloader(),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcsng-cuni\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis/runs/jcnvje25\" target=\"_blank\">astral-elevator-7</a></strong> to <a href=\"https://wandb.ai/csng-cuni/reCNN_visual_prosthesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/callbacks/progress/progress.py:21: LightningDeprecationWarning: `ProgressBar` has been deprecated in v1.5 and will be removed in v1.7. It has been renamed to `TQDMProgressBar` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 10,\n",
      " 'bottleneck_kernel': 15,\n",
      " 'compute_oracle_fraction': False,\n",
      " 'conservative_oracle': True,\n",
      " 'core_gamma_hidden': 0.28463619129195233,\n",
      " 'core_gamma_input': 0.00307424496692959,\n",
      " 'core_hidden_channels': 8,\n",
      " 'core_hidden_kern': 9,\n",
      " 'core_input_kern': 7,\n",
      " 'core_layers': 5,\n",
      " 'dataset_artifact_name': 'Antolik_dataset:latest',\n",
      " 'depth_separable': True,\n",
      " 'do_not_sample': True,\n",
      " 'factor': 5.5,\n",
      " 'filtered_neurons': None,\n",
      " 'fixed_sigma': False,\n",
      " 'freeze_orientations': False,\n",
      " 'freeze_positions': False,\n",
      " 'generate_oracle_figure': False,\n",
      " 'ground_truth_orientations_file_path': 'data/antolik/oris_reparametrized.pickle',\n",
      " 'ground_truth_positions_file_path': 'data/antolik/positions_reparametrized.pickle',\n",
      " 'init_mu_range': 0.9,\n",
      " 'init_sigma_range': 0.8,\n",
      " 'init_to_ground_truth_orientations': False,\n",
      " 'init_to_ground_truth_positions': False,\n",
      " 'input_channels': 1,\n",
      " 'input_regularizer': 'LaplaceL2norm',\n",
      " 'input_size_x': 110,\n",
      " 'input_size_y': 110,\n",
      " 'jackknife_oracle': True,\n",
      " 'lr': 0.01,\n",
      " 'max_epochs': 100,\n",
      " 'mean_activity': '[ 7.792459   0.8011908  4.5714283 ... 13.284128   '\n",
      "                  '3.130158   4.6115093]',\n",
      " 'nonlinearity': 'softplus',\n",
      " 'num_neurons': 5000,\n",
      " 'num_rotations': 8,\n",
      " 'observed_val_metric': 'val/corr',\n",
      " 'orientation_shift': 87.4,\n",
      " 'patience': 7,\n",
      " 'positions_minus_x': False,\n",
      " 'positions_minus_y': True,\n",
      " 'readout_bias': False,\n",
      " 'readout_gamma': 0.17,\n",
      " 'reg_group_sparsity': 0.1,\n",
      " 'reg_readout_spatial_smoothness': 0.0027,\n",
      " 'reg_spatial_sparsity': 0.45,\n",
      " 'region': 'region1',\n",
      " 'rot_eq_batch_norm': True,\n",
      " 'sample': False,\n",
      " 'seed': 2,\n",
      " 'stack': -1,\n",
      " 'stride': 1,\n",
      " 'test': True,\n",
      " 'test_average_batch': False,\n",
      " 'train_on_val': False,\n",
      " 'upsampling': 2,\n",
      " 'use_avg_reg': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-fe783ca2-3d3f-1409-1ba8-3ae437d45687]\n",
      "\n",
      "  | Name    | Type                                | Params\n",
      "----------------------------------------------------------------\n",
      "0 | loss    | PoissonLoss                         | 0     \n",
      "1 | corr    | Corr                                | 0     \n",
      "2 | core    | RotationEquivariant2dCoreBottleneck | 458 K \n",
      "3 | readout | Gaussian3dCyclicNoScale             | 35.0 K\n",
      "4 | nonlin  | Softplus                            | 0     \n",
      "----------------------------------------------------------------\n",
      "135 K     Trainable params\n",
      "358 K     Non-trainable params\n",
      "493 K     Total params\n",
      "1.973     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:452: UserWarning: Your `val_dataloader` has `shuffle=True`,it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  17%|█▋        | 83/500 [00:13<01:06,  6.26it/s, loss=-0.401] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-48aedca083f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     best_observed_val_metric = (\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     76\u001b[0m     print(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from Lurz_dataset import LurzDataModule\n",
    "\n",
    "from models import reCNN_FullFactorized\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import ProgressBar\n",
    "import pytorch_lightning as pl\n",
    "from models import reCNN_bottleneck_CyclicGauss3d\n",
    "from pprint import pprint\n",
    "from Antolik_dataset import AntolikDataModule\n",
    "\n",
    "if use_wandb:\n",
    "\n",
    "    pl.seed_everything(config[\"seed\"], workers=True)    \n",
    "\n",
    "    # init wandb run\n",
    "    run = wandb.init(\n",
    "        config=config,\n",
    "        project=PROJECT,\n",
    "        entity=ENTITY,\n",
    "    )\n",
    "\n",
    "    # Access all hyperparameter values through wandb.config\n",
    "    config = dict(wandb.config)\n",
    "    pprint(config)\n",
    "\n",
    "\n",
    "    # setup wandb logger\n",
    "    wandb_logger = WandbLogger(log_model=True)\n",
    "    wandb_logger.watch(model, log=\"parameters\", log_freq=250)\n",
    "\n",
    "    # define callbacks for the training\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=early_stopping_monitor,\n",
    "        patience=config[\"patience\"],\n",
    "        mode=early_stopping_mode,\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=1, monitor=model_checkpoint_monitor, mode=model_checkpoint_mode\n",
    "    )\n",
    "\n",
    "    class LitProgressBar(ProgressBar):\n",
    "        def get_metrics(self, trainer, model):\n",
    "            # don't show the version number\n",
    "            items = super().get_metrics(trainer, model)\n",
    "            items.pop(\"v_num\", None)\n",
    "            return items\n",
    "\n",
    "    bar = LitProgressBar()\n",
    "\n",
    "\n",
    "    # define the trainer\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[early_stop, checkpoint_callback, bar],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        # max_epochs=1,\n",
    "        gpus=[0],\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=250,\n",
    "        # deterministic=True,\n",
    "        enable_checkpointing=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=dm.train_dataloader(),\n",
    "        val_dataloaders=dm.val_dataloader(),\n",
    "        )\n",
    "\n",
    "    best_observed_val_metric = (\n",
    "        checkpoint_callback.best_model_score.cpu().detach().numpy()\n",
    "    )\n",
    "    print(\n",
    "        \"Best model's \"\n",
    "        + config[\"observed_val_metric\"]\n",
    "        + \": \"\n",
    "        + str(best_observed_val_metric)\n",
    "    )\n",
    "\n",
    "    if model_artifact_name == None:\n",
    "        model_artifact_name = model.__str__()\n",
    "    \n",
    "    print(model_artifact_name)\n",
    "    print(model_artifact_name)\n",
    "\n",
    "    # add best corr to metadata\n",
    "    metadata = {**config, \"best_model_score\": best_observed_val_metric}\n",
    "\n",
    "    # add model artifact\n",
    "    best_model_artifact = wandb.Artifact(\n",
    "        model_artifact_name, type=\"model\", metadata=metadata\n",
    "    )\n",
    "    print(best_model_artifact)\n",
    "    print(best_model_artifact)\n",
    "    best_model_artifact.add_file(checkpoint_callback.best_model_path)\n",
    "    run.log_artifact(best_model_artifact)\n",
    "\n",
    "    # say to wandb that the best val/corr of the model is the best one\n",
    "    # and not the last one!! (it is the default behavour!!)\n",
    "    run.summary[config[\"observed_val_metric\"]] = best_observed_val_metric\n",
    "\n",
    "    print(checkpoint_callback.best_model_path)\n",
    "\n",
    "    model = model_class.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n",
    "    if config[\"test\"]:\n",
    "        dm.model_performances(model, trainer)\n",
    "\n",
    "\n",
    "\n",
    "        # result_artifact = wandb.Artifact(name=\"RESULT_\" + model_artifact_name, type=\"result\",\n",
    "        #     metadata=results[0])\n",
    "        # run.log_artifact(result_artifact)\n",
    "\n",
    "else:\n",
    "    pl.seed_everything(config[\"seed\"], workers=True)\n",
    "    pprint(config)\n",
    "\n",
    "\n",
    "    # define callbacks for the training\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=early_stopping_monitor,\n",
    "        patience=config[\"patience\"],\n",
    "        mode=early_stopping_mode,\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=1, monitor=model_checkpoint_monitor, mode=model_checkpoint_mode\n",
    "    )\n",
    "\n",
    "    class LitProgressBar(ProgressBar):\n",
    "        def get_metrics(self, trainer, model):\n",
    "            # don't show the version number\n",
    "            items = super().get_metrics(trainer, model)\n",
    "            items.pop(\"v_num\", None)\n",
    "            return items\n",
    "\n",
    "    bar = LitProgressBar()\n",
    "\n",
    "    # define the trainer\n",
    "    trainer = pl.Trainer(\n",
    "        callbacks=[early_stop, checkpoint_callback, bar],\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        gpus=[0],\n",
    "        # logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        # deterministic=True,\n",
    "        enable_checkpointing=True,\n",
    "    )\n",
    "\n",
    "    if config[\"train_on_val\"]:\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders=dm.val_dataloader(),\n",
    "            val_dataloaders=dm.val_dataloader(),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders=dm.train_dataloader(),\n",
    "            val_dataloaders=dm.val_dataloader(),\n",
    "        )\n",
    "\n",
    "    best_observed_val_metric = (\n",
    "        checkpoint_callback.best_model_score.cpu().detach().numpy()\n",
    "    )\n",
    "    print(\n",
    "        \"Best model's \"\n",
    "        + config[\"observed_val_metric\"]\n",
    "        + \": \"\n",
    "        + str(best_observed_val_metric)\n",
    "    )\n",
    "\n",
    "    # add best corr to metadata\n",
    "    metadata = {**config, \"best_model_score\": best_observed_val_metric}\n",
    "\n",
    "    print(checkpoint_callback.best_model_path)\n",
    "\n",
    "    model = model_class.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n",
    "    if config[\"test\"]:\n",
    "        dm.model_performances(model, trainer)\n",
    "\n",
    "        # result_artifact = wandb.Artifact(name=\"RESULT_\" + model_artifact_name, type=\"result\",\n",
    "        #     metadata=results[0])\n",
    "        # run.log_artifact(result_artifact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
